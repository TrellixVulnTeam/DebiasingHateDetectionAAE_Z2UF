{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import sys\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "root = '../src/baselines/runs/'\n",
    "models = ['bert_oc_gs', 'bert_soc_gs']\n",
    "datasets = ['davidson', 'founta', 'golbeck', 'harassment', 'hate']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# gridsearch aggregate\n",
    "result = None\n",
    "for model in models:\n",
    "    for dataset in datasets:\n",
    "        for subdir in os.listdir(f'{root}/{model}'):\n",
    "            if os.path.exists(f'{root}/{model}/{subdir}/eval_results_0_dev_{dataset}.txt'):\n",
    "                f = pd.read_csv(\n",
    "                    filepath_or_buffer=f'{root}{model}/{subdir}/eval_results_0_dev_{dataset}.txt',\n",
    "                    delimiter='=',\n",
    "                    # names=['metric','score'],\n",
    "                    header=None,\n",
    "                )\n",
    "                args = pd.read_json(f'{root}{model}/{subdir}/args.json', typ='series')\n",
    "                args = args.to_frame().reset_index().rename(columns={\"index\": 0, 0: 1})\n",
    "                f = pd.concat([f, args], ignore_index=True)\n",
    "                f = f.append([['model', model[:-3]]], ignore_index=True)\n",
    "                f = f.append([['dataset', dataset]], ignore_index=True)\n",
    "                f.set_index(0, inplace=True)\n",
    "                f = f.transpose()\n",
    "                if result is None:\n",
    "                    result = f\n",
    "                else:\n",
    "                    result = pd.concat([result, f], ignore_index=True, sort=False)\n",
    "            else: pass\n",
    "\n",
    "\n",
    "result.to_csv(f'{root}/gs_results/gridsearch_expl_results_raw.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "0       acc   auc_roc  disparate_impact_favorable_06   \\\n0    0.93866  0.972684                       0.236528   \n1    0.94431   0.97097                       0.121867   \n2   0.957627  0.981972                       0.130753   \n3   0.964487  0.987828                      0.0964078   \n4   0.961259  0.986236                       0.131026   \n..       ...       ...                            ...   \n80  0.939866  0.891043                        1.02129   \n81  0.940189  0.890779                        1.02529   \n82   0.94148  0.893815                        1.01529   \n83  0.941561  0.895321                        1.02177   \n84  0.938897  0.885384                        1.01993   \n\n0  disparate_impact_favorable_08  disparate_impact_unfavorable_06   \\\n0                        0.426022                          1.26339   \n1                       0.0864928                          1.30454   \n2                       0.0926209                           1.2745   \n3                        0.103284                          1.25042   \n4                       0.0928088                          1.27366   \n..                            ...                              ...   \n80                        1.02923                         0.292957   \n81                         1.0313                          0.21853   \n82                        1.01515                         0.509598   \n83                        1.03329                         0.362816   \n84                        1.02615                         0.260826   \n\n0  disparate_impact_unfavorable_08  eval_loss  eval_loss_reg        f1   \\\n0                           1.16326   0.168889    4.01722e-05  0.961771   \n1                           1.25488   0.158455    0.000468094  0.965396   \n2                           1.23213    0.13663    0.000138618    0.9739   \n3                           1.20043   0.100869     0.00132101    0.9784   \n4                           1.23149   0.107736     0.00142147  0.976143   \n..                              ...        ...            ...       ...   \n80                                0   0.166691     0.00137342  0.389844   \n81                                0   0.167264     0.00150361  0.404819   \n82                         0.503864   0.164127     0.00110855  0.413905   \n83                                0   0.164603     0.00127491  0.429022   \n84                                0   0.171447     0.00171813  0.361181   \n\n0  fnr_priv_06   ... local_rank seed gradient_accumulation_steps   fp16  \\\n0     0.0704762  ...         -1   42                           1  False   \n1     0.0692063  ...         -1   42                           1  False   \n2     0.0495238  ...         -1   42                           1  False   \n3     0.0285714  ...         -1   42                           1  False   \n4     0.0463492  ...         -1   42                           1  False   \n..          ...  ...        ...  ...                         ...    ...   \n80      0.71875  ...         -1   42                           1  False   \n81     0.701923  ...         -1   42                           1  False   \n82     0.699519  ...         -1   42                           1  False   \n83     0.679087  ...         -1   42                           1  False   \n84      0.74399  ...         -1   42                           1  False   \n\n0  loss_scale server_ip server_port continue_from_checkpoint     model  \\\n0           0                                              0   bert_oc   \n1           0                                              0   bert_oc   \n2           0                                              0   bert_oc   \n3           0                                              0   bert_oc   \n4           0                                              0   bert_oc   \n..        ...       ...         ...                      ...       ...   \n80          0                                              0  bert_soc   \n81          0                                              0  bert_soc   \n82          0                                              0  bert_soc   \n83          0                                              0  bert_soc   \n84          0                                              0  bert_soc   \n\n0    dataset  \n0   davidson  \n1   davidson  \n2   davidson  \n3   davidson  \n4   davidson  \n..       ...  \n80      hate  \n81      hate  \n82      hate  \n83      hate  \n84      hate  \n\n[85 rows x 98 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>acc</th>\n      <th>auc_roc</th>\n      <th>disparate_impact_favorable_06</th>\n      <th>disparate_impact_favorable_08</th>\n      <th>disparate_impact_unfavorable_06</th>\n      <th>disparate_impact_unfavorable_08</th>\n      <th>eval_loss</th>\n      <th>eval_loss_reg</th>\n      <th>f1</th>\n      <th>fnr_priv_06</th>\n      <th>...</th>\n      <th>local_rank</th>\n      <th>seed</th>\n      <th>gradient_accumulation_steps</th>\n      <th>fp16</th>\n      <th>loss_scale</th>\n      <th>server_ip</th>\n      <th>server_port</th>\n      <th>continue_from_checkpoint</th>\n      <th>model</th>\n      <th>dataset</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.93866</td>\n      <td>0.972684</td>\n      <td>0.236528</td>\n      <td>0.426022</td>\n      <td>1.26339</td>\n      <td>1.16326</td>\n      <td>0.168889</td>\n      <td>4.01722e-05</td>\n      <td>0.961771</td>\n      <td>0.0704762</td>\n      <td>...</td>\n      <td>-1</td>\n      <td>42</td>\n      <td>1</td>\n      <td>False</td>\n      <td>0</td>\n      <td></td>\n      <td></td>\n      <td>0</td>\n      <td>bert_oc</td>\n      <td>davidson</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.94431</td>\n      <td>0.97097</td>\n      <td>0.121867</td>\n      <td>0.0864928</td>\n      <td>1.30454</td>\n      <td>1.25488</td>\n      <td>0.158455</td>\n      <td>0.000468094</td>\n      <td>0.965396</td>\n      <td>0.0692063</td>\n      <td>...</td>\n      <td>-1</td>\n      <td>42</td>\n      <td>1</td>\n      <td>False</td>\n      <td>0</td>\n      <td></td>\n      <td></td>\n      <td>0</td>\n      <td>bert_oc</td>\n      <td>davidson</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.957627</td>\n      <td>0.981972</td>\n      <td>0.130753</td>\n      <td>0.0926209</td>\n      <td>1.2745</td>\n      <td>1.23213</td>\n      <td>0.13663</td>\n      <td>0.000138618</td>\n      <td>0.9739</td>\n      <td>0.0495238</td>\n      <td>...</td>\n      <td>-1</td>\n      <td>42</td>\n      <td>1</td>\n      <td>False</td>\n      <td>0</td>\n      <td></td>\n      <td></td>\n      <td>0</td>\n      <td>bert_oc</td>\n      <td>davidson</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.964487</td>\n      <td>0.987828</td>\n      <td>0.0964078</td>\n      <td>0.103284</td>\n      <td>1.25042</td>\n      <td>1.20043</td>\n      <td>0.100869</td>\n      <td>0.00132101</td>\n      <td>0.9784</td>\n      <td>0.0285714</td>\n      <td>...</td>\n      <td>-1</td>\n      <td>42</td>\n      <td>1</td>\n      <td>False</td>\n      <td>0</td>\n      <td></td>\n      <td></td>\n      <td>0</td>\n      <td>bert_oc</td>\n      <td>davidson</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.961259</td>\n      <td>0.986236</td>\n      <td>0.131026</td>\n      <td>0.0928088</td>\n      <td>1.27366</td>\n      <td>1.23149</td>\n      <td>0.107736</td>\n      <td>0.00142147</td>\n      <td>0.976143</td>\n      <td>0.0463492</td>\n      <td>...</td>\n      <td>-1</td>\n      <td>42</td>\n      <td>1</td>\n      <td>False</td>\n      <td>0</td>\n      <td></td>\n      <td></td>\n      <td>0</td>\n      <td>bert_oc</td>\n      <td>davidson</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>80</th>\n      <td>0.939866</td>\n      <td>0.891043</td>\n      <td>1.02129</td>\n      <td>1.02923</td>\n      <td>0.292957</td>\n      <td>0</td>\n      <td>0.166691</td>\n      <td>0.00137342</td>\n      <td>0.389844</td>\n      <td>0.71875</td>\n      <td>...</td>\n      <td>-1</td>\n      <td>42</td>\n      <td>1</td>\n      <td>False</td>\n      <td>0</td>\n      <td></td>\n      <td></td>\n      <td>0</td>\n      <td>bert_soc</td>\n      <td>hate</td>\n    </tr>\n    <tr>\n      <th>81</th>\n      <td>0.940189</td>\n      <td>0.890779</td>\n      <td>1.02529</td>\n      <td>1.0313</td>\n      <td>0.21853</td>\n      <td>0</td>\n      <td>0.167264</td>\n      <td>0.00150361</td>\n      <td>0.404819</td>\n      <td>0.701923</td>\n      <td>...</td>\n      <td>-1</td>\n      <td>42</td>\n      <td>1</td>\n      <td>False</td>\n      <td>0</td>\n      <td></td>\n      <td></td>\n      <td>0</td>\n      <td>bert_soc</td>\n      <td>hate</td>\n    </tr>\n    <tr>\n      <th>82</th>\n      <td>0.94148</td>\n      <td>0.893815</td>\n      <td>1.01529</td>\n      <td>1.01515</td>\n      <td>0.509598</td>\n      <td>0.503864</td>\n      <td>0.164127</td>\n      <td>0.00110855</td>\n      <td>0.413905</td>\n      <td>0.699519</td>\n      <td>...</td>\n      <td>-1</td>\n      <td>42</td>\n      <td>1</td>\n      <td>False</td>\n      <td>0</td>\n      <td></td>\n      <td></td>\n      <td>0</td>\n      <td>bert_soc</td>\n      <td>hate</td>\n    </tr>\n    <tr>\n      <th>83</th>\n      <td>0.941561</td>\n      <td>0.895321</td>\n      <td>1.02177</td>\n      <td>1.03329</td>\n      <td>0.362816</td>\n      <td>0</td>\n      <td>0.164603</td>\n      <td>0.00127491</td>\n      <td>0.429022</td>\n      <td>0.679087</td>\n      <td>...</td>\n      <td>-1</td>\n      <td>42</td>\n      <td>1</td>\n      <td>False</td>\n      <td>0</td>\n      <td></td>\n      <td></td>\n      <td>0</td>\n      <td>bert_soc</td>\n      <td>hate</td>\n    </tr>\n    <tr>\n      <th>84</th>\n      <td>0.938897</td>\n      <td>0.885384</td>\n      <td>1.01993</td>\n      <td>1.02615</td>\n      <td>0.260826</td>\n      <td>0</td>\n      <td>0.171447</td>\n      <td>0.00171813</td>\n      <td>0.361181</td>\n      <td>0.74399</td>\n      <td>...</td>\n      <td>-1</td>\n      <td>42</td>\n      <td>1</td>\n      <td>False</td>\n      <td>0</td>\n      <td></td>\n      <td></td>\n      <td>0</td>\n      <td>bert_soc</td>\n      <td>hate</td>\n    </tr>\n  </tbody>\n</table>\n<p>85 rows × 98 columns</p>\n</div>"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "'bert_soc'"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'bert_soc_gs'[:-3]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    for dataset in datasets:\n",
    "        temp = result[(result['dataset']==dataset) & (result['model'] == model[:-3])]\n",
    "        temp = temp.sort_values(by=['f1 '], ascending=False)\n",
    "        temp.to_csv(f'{root}/gs_results/{model[:-3]}_{dataset}.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "best_results_gs = None\n",
    "for model in models:\n",
    "    for dataset in datasets:\n",
    "        if best_results_gs is None:\n",
    "            best_results_gs = pd.read_csv(f'{root}/gs_results/{model[:-3]}_{dataset}.csv').head(1)\n",
    "        else:\n",
    "           best_results_gs = pd.concat([best_results_gs, pd.read_csv(f'{root}/gs_results/{model[:-3]}_{dataset}.csv').head(1)])\n",
    "best_results_gs.to_csv(f'{root}/gs_results/best_hp_results_expl.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "       acc   auc_roc   disparate_impact_favorable_06   \\\n0  0.967312  0.988467                        0.135910   \n0  0.939206  0.972975                        0.185922   \n0  0.791582  0.725867                        0.934984   \n0  0.903094  0.955676                        0.083677   \n0  0.940189  0.889739                        1.022384   \n0  0.970137  0.989351                        0.125620   \n0  0.938880  0.972734                        0.176721   \n0  0.784990  0.725321                        0.583135   \n0  0.908895  0.959597                        0.086479   \n0  0.941561  0.895321                        1.021770   \n\n   disparate_impact_favorable_08   disparate_impact_unfavorable_06   \\\n0                        0.103051                          1.237363   \n0                        0.137582                          3.260271   \n0                        1.068872                          1.948413   \n0                        0.059509                          2.810277   \n0                        1.024954                          0.135665   \n0                        0.102820                          1.241612   \n0                        0.138058                          3.257554   \n0                        1.168346                          3.507143   \n0                        0.019939                          2.775356   \n0                        1.033291                          0.362816   \n\n   disparate_impact_unfavorable_08   eval_loss   eval_loss_reg        f1   \\\n0                          1.201032    0.096138        0.001522  0.980113   \n0                          3.294739    0.177459        0.002693  0.889416   \n0                          0.000000    0.480020        0.000165  0.318408   \n0                          2.658547    0.257533        0.001100  0.872017   \n0                          0.000000    0.167939        0.001446  0.367208   \n0                          1.201639    0.104425        0.000818  0.981827   \n0                          3.264810    0.176269        0.002505  0.889327   \n0                          0.000000    0.515957        0.000545  0.442105   \n0                          2.703913    0.245078        0.000653  0.880219   \n0                          0.000000    0.164603        0.001275  0.429022   \n\n   fnr_priv_06   ...  local_rank  seed  gradient_accumulation_steps   fp16  \\\n0      0.026032  ...          -1    42                            1  False   \n0      0.118084  ...          -1    42                            1  False   \n0      0.798301  ...          -1    42                            1  False   \n0      0.175424  ...          -1    42                            1  False   \n0      0.743990  ...          -1    42                            1  False   \n0      0.024762  ...          -1    42                            1  False   \n0      0.114368  ...          -1    42                            1  False   \n0      0.649682  ...          -1    42                            1  False   \n0      0.162342  ...          -1    42                            1  False   \n0      0.679087  ...          -1    42                            1  False   \n\n   loss_scale  server_ip  server_port  continue_from_checkpoint     model  \\\n0           0        NaN          NaN                         0   bert_oc   \n0           0        NaN          NaN                         0   bert_oc   \n0           0        NaN          NaN                         0   bert_oc   \n0           0        NaN          NaN                         0   bert_oc   \n0           0        NaN          NaN                         0   bert_oc   \n0           0        NaN          NaN                         0  bert_soc   \n0           0        NaN          NaN                         0  bert_soc   \n0           0        NaN          NaN                         0  bert_soc   \n0           0        NaN          NaN                         0  bert_soc   \n0           0        NaN          NaN                         0  bert_soc   \n\n      dataset  \n0    davidson  \n0      founta  \n0     golbeck  \n0  harassment  \n0        hate  \n0    davidson  \n0      founta  \n0     golbeck  \n0  harassment  \n0        hate  \n\n[10 rows x 98 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>acc</th>\n      <th>auc_roc</th>\n      <th>disparate_impact_favorable_06</th>\n      <th>disparate_impact_favorable_08</th>\n      <th>disparate_impact_unfavorable_06</th>\n      <th>disparate_impact_unfavorable_08</th>\n      <th>eval_loss</th>\n      <th>eval_loss_reg</th>\n      <th>f1</th>\n      <th>fnr_priv_06</th>\n      <th>...</th>\n      <th>local_rank</th>\n      <th>seed</th>\n      <th>gradient_accumulation_steps</th>\n      <th>fp16</th>\n      <th>loss_scale</th>\n      <th>server_ip</th>\n      <th>server_port</th>\n      <th>continue_from_checkpoint</th>\n      <th>model</th>\n      <th>dataset</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.967312</td>\n      <td>0.988467</td>\n      <td>0.135910</td>\n      <td>0.103051</td>\n      <td>1.237363</td>\n      <td>1.201032</td>\n      <td>0.096138</td>\n      <td>0.001522</td>\n      <td>0.980113</td>\n      <td>0.026032</td>\n      <td>...</td>\n      <td>-1</td>\n      <td>42</td>\n      <td>1</td>\n      <td>False</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>bert_oc</td>\n      <td>davidson</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>0.939206</td>\n      <td>0.972975</td>\n      <td>0.185922</td>\n      <td>0.137582</td>\n      <td>3.260271</td>\n      <td>3.294739</td>\n      <td>0.177459</td>\n      <td>0.002693</td>\n      <td>0.889416</td>\n      <td>0.118084</td>\n      <td>...</td>\n      <td>-1</td>\n      <td>42</td>\n      <td>1</td>\n      <td>False</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>bert_oc</td>\n      <td>founta</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>0.791582</td>\n      <td>0.725867</td>\n      <td>0.934984</td>\n      <td>1.068872</td>\n      <td>1.948413</td>\n      <td>0.000000</td>\n      <td>0.480020</td>\n      <td>0.000165</td>\n      <td>0.318408</td>\n      <td>0.798301</td>\n      <td>...</td>\n      <td>-1</td>\n      <td>42</td>\n      <td>1</td>\n      <td>False</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>bert_oc</td>\n      <td>golbeck</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>0.903094</td>\n      <td>0.955676</td>\n      <td>0.083677</td>\n      <td>0.059509</td>\n      <td>2.810277</td>\n      <td>2.658547</td>\n      <td>0.257533</td>\n      <td>0.001100</td>\n      <td>0.872017</td>\n      <td>0.175424</td>\n      <td>...</td>\n      <td>-1</td>\n      <td>42</td>\n      <td>1</td>\n      <td>False</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>bert_oc</td>\n      <td>harassment</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>0.940189</td>\n      <td>0.889739</td>\n      <td>1.022384</td>\n      <td>1.024954</td>\n      <td>0.135665</td>\n      <td>0.000000</td>\n      <td>0.167939</td>\n      <td>0.001446</td>\n      <td>0.367208</td>\n      <td>0.743990</td>\n      <td>...</td>\n      <td>-1</td>\n      <td>42</td>\n      <td>1</td>\n      <td>False</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>bert_oc</td>\n      <td>hate</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>0.970137</td>\n      <td>0.989351</td>\n      <td>0.125620</td>\n      <td>0.102820</td>\n      <td>1.241612</td>\n      <td>1.201639</td>\n      <td>0.104425</td>\n      <td>0.000818</td>\n      <td>0.981827</td>\n      <td>0.024762</td>\n      <td>...</td>\n      <td>-1</td>\n      <td>42</td>\n      <td>1</td>\n      <td>False</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>bert_soc</td>\n      <td>davidson</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>0.938880</td>\n      <td>0.972734</td>\n      <td>0.176721</td>\n      <td>0.138058</td>\n      <td>3.257554</td>\n      <td>3.264810</td>\n      <td>0.176269</td>\n      <td>0.002505</td>\n      <td>0.889327</td>\n      <td>0.114368</td>\n      <td>...</td>\n      <td>-1</td>\n      <td>42</td>\n      <td>1</td>\n      <td>False</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>bert_soc</td>\n      <td>founta</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>0.784990</td>\n      <td>0.725321</td>\n      <td>0.583135</td>\n      <td>1.168346</td>\n      <td>3.507143</td>\n      <td>0.000000</td>\n      <td>0.515957</td>\n      <td>0.000545</td>\n      <td>0.442105</td>\n      <td>0.649682</td>\n      <td>...</td>\n      <td>-1</td>\n      <td>42</td>\n      <td>1</td>\n      <td>False</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>bert_soc</td>\n      <td>golbeck</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>0.908895</td>\n      <td>0.959597</td>\n      <td>0.086479</td>\n      <td>0.019939</td>\n      <td>2.775356</td>\n      <td>2.703913</td>\n      <td>0.245078</td>\n      <td>0.000653</td>\n      <td>0.880219</td>\n      <td>0.162342</td>\n      <td>...</td>\n      <td>-1</td>\n      <td>42</td>\n      <td>1</td>\n      <td>False</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>bert_soc</td>\n      <td>harassment</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>0.941561</td>\n      <td>0.895321</td>\n      <td>1.021770</td>\n      <td>1.033291</td>\n      <td>0.362816</td>\n      <td>0.000000</td>\n      <td>0.164603</td>\n      <td>0.001275</td>\n      <td>0.429022</td>\n      <td>0.679087</td>\n      <td>...</td>\n      <td>-1</td>\n      <td>42</td>\n      <td>1</td>\n      <td>False</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>bert_soc</td>\n      <td>hate</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows × 98 columns</p>\n</div>"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_results_gs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     object\n",
      "1    float64\n",
      "dtype: object\n",
      "0\n",
      "acc                                 float64\n",
      "auc_roc                             float64\n",
      "disparate_impact_favorable_06       float64\n",
      "disparate_impact_favorable_08       float64\n",
      "disparate_impact_unfavorable_06     float64\n",
      "disparate_impact_unfavorable_08     float64\n",
      "f1                                  float64\n",
      "fnr_priv_06                         float64\n",
      "fnr_priv_08                         float64\n",
      "fnr_total_06                        float64\n",
      "fnr_total_08                        float64\n",
      "fnr_unpriv_06                       float64\n",
      "fnr_unpriv_08                       float64\n",
      "fpr_priv_06                         float64\n",
      "fpr_priv_08                         float64\n",
      "fpr_total_06                        float64\n",
      "fpr_total_08                        float64\n",
      "fpr_unpriv_06                       float64\n",
      "fpr_unpriv_08                       float64\n",
      "loss                                float64\n",
      "precision                           float64\n",
      "priv_n_06                           float64\n",
      "priv_n_08                           float64\n",
      "priv_ratio_favorable_06             float64\n",
      "priv_ratio_favorable_08             float64\n",
      "priv_ratio_unfavorable_06           float64\n",
      "priv_ratio_unfavorable_08           float64\n",
      "priv_total_06                       float64\n",
      "priv_total_08                       float64\n",
      "recall                              float64\n",
      "unpriv_n_06                         float64\n",
      "unpriv_n_08                         float64\n",
      "unpriv_ratio_favorable_06           float64\n",
      "unpriv_ratio_favorable_08           float64\n",
      "unpriv_ratio_unfavorable_06         float64\n",
      "unpriv_ratio_unfavorable_08         float64\n",
      "unpriv_total_06                     float64\n",
      "unpriv_total_08                     float64\n",
      "is_local                             object\n",
      "is_gcloud                            object\n",
      "is_gridsearch                        object\n",
      "retrain                              object\n",
      "task_name                            object\n",
      "seed                                 object\n",
      "dataset                              object\n",
      "batch_size                           object\n",
      "epochs                               object\n",
      "max_seq_length                       object\n",
      "learning_rate                        object\n",
      "reg_strength                         object\n",
      "class_weight                         object\n",
      "output_dir                           object\n",
      "do_train                             object\n",
      "do_eval                              object\n",
      "test                                 object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "f = pd.read_csv(\n",
    "                    filepath_or_buffer=f'{root}log_reg_results/glove_davidson_0/eval_results_dev_glove',\n",
    "                    delimiter='=',\n",
    "                    header=None,\n",
    "                )\n",
    "f[1] = f[1].astype(float)\n",
    "print(f.dtypes)\n",
    "args = pd.read_json(f'{root}log_reg_results/glove_davidson_0/args.json', typ='series', dtype=str)\n",
    "args = args.to_frame().reset_index().rename(columns={\"index\": 0, 0: 1})\n",
    "f = pd.concat([f, args], ignore_index=True)\n",
    "f.set_index(0, inplace=True)\n",
    "f = f.transpose()\n",
    "\n",
    "f[['acc ', 'auc_roc ', 'disparate_impact_favorable_06 ',\n",
    "       'disparate_impact_favorable_08 ', 'disparate_impact_unfavorable_06 ',\n",
    "       'disparate_impact_unfavorable_08 ', 'f1 ', 'fnr_priv_06 ',\n",
    "       'fnr_priv_08 ', 'fnr_total_06 ', 'fnr_total_08 ', 'fnr_unpriv_06 ',\n",
    "       'fnr_unpriv_08 ', 'fpr_priv_06 ', 'fpr_priv_08 ', 'fpr_total_06 ',\n",
    "       'fpr_total_08 ', 'fpr_unpriv_06 ', 'fpr_unpriv_08 ', 'loss ',\n",
    "       'precision ', 'priv_n_06 ', 'priv_n_08 ', 'priv_ratio_favorable_06 ',\n",
    "       'priv_ratio_favorable_08 ', 'priv_ratio_unfavorable_06 ',\n",
    "       'priv_ratio_unfavorable_08 ', 'priv_total_06 ', 'priv_total_08 ',\n",
    "       'recall ', 'unpriv_n_06 ', 'unpriv_n_08 ', 'unpriv_ratio_favorable_06 ',\n",
    "       'unpriv_ratio_favorable_08 ', 'unpriv_ratio_unfavorable_06 ',\n",
    "       'unpriv_ratio_unfavorable_08 ', 'unpriv_total_06 ', 'unpriv_total_08 ']] = f[['acc ', 'auc_roc ', 'disparate_impact_favorable_06 ',\n",
    "       'disparate_impact_favorable_08 ', 'disparate_impact_unfavorable_06 ',\n",
    "       'disparate_impact_unfavorable_08 ', 'f1 ', 'fnr_priv_06 ',\n",
    "       'fnr_priv_08 ', 'fnr_total_06 ', 'fnr_total_08 ', 'fnr_unpriv_06 ',\n",
    "       'fnr_unpriv_08 ', 'fpr_priv_06 ', 'fpr_priv_08 ', 'fpr_total_06 ',\n",
    "       'fpr_total_08 ', 'fpr_unpriv_06 ', 'fpr_unpriv_08 ', 'loss ',\n",
    "       'precision ', 'priv_n_06 ', 'priv_n_08 ', 'priv_ratio_favorable_06 ',\n",
    "       'priv_ratio_favorable_08 ', 'priv_ratio_unfavorable_06 ',\n",
    "       'priv_ratio_unfavorable_08 ', 'priv_total_06 ', 'priv_total_08 ',\n",
    "       'recall ', 'unpriv_n_06 ', 'unpriv_n_08 ', 'unpriv_ratio_favorable_06 ',\n",
    "       'unpriv_ratio_favorable_08 ', 'unpriv_ratio_unfavorable_06 ',\n",
    "       'unpriv_ratio_unfavorable_08 ', 'unpriv_total_06 ', 'unpriv_total_08 ',]].astype(float)\n",
    "\n",
    "print(f.dtypes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "# seeds aggregate\n",
    "\n",
    "result_val = None\n",
    "result_test = None\n",
    "pattern_dev = 'eval_results_dev'\n",
    "pattern_test = 'eval_results_test'\n",
    "for subdir in os.listdir(f'{root}log_reg_results'):\n",
    "    for file in os.listdir(f'{root}log_reg_results/{subdir}/'):\n",
    "        if re.match(pattern_dev, file):\n",
    "            f = pd.read_csv(\n",
    "                    filepath_or_buffer=f'{root}log_reg_results/{subdir}/{file}',\n",
    "                    delimiter='=',\n",
    "                    header=None,\n",
    "                )\n",
    "            f[1] = f[1].astype(float)\n",
    "            args = pd.read_json(f'{root}log_reg_results/{subdir}/args.json', typ='series', dtype=str)\n",
    "            args = args.to_frame().reset_index().rename(columns={\"index\": 0, 0: 1})\n",
    "            f = pd.concat([f, args], ignore_index=True)\n",
    "            f.set_index(0, inplace=True)\n",
    "            f = f.transpose()\n",
    "            if result_val is None:\n",
    "                result_val = f\n",
    "            else:\n",
    "                result_val = pd.concat([result_val, f], ignore_index=True, sort=False)\n",
    "        elif re.match(pattern_test, file):\n",
    "            f = pd.read_csv(\n",
    "                    filepath_or_buffer=f'{root}log_reg_results/{subdir}/{file}',\n",
    "                    delimiter='=',\n",
    "                    header=None,\n",
    "            )\n",
    "            f[1] = f[1].astype(float)\n",
    "            args = pd.read_json(f'{root}log_reg_results/{subdir}/args.json', typ='series', dtype=str)\n",
    "            args = args.to_frame().reset_index().rename(columns={\"index\": 0, 0: 1})\n",
    "            f = pd.concat([f, args], ignore_index=True)\n",
    "            f.set_index(0, inplace=True)\n",
    "            f = f.transpose()\n",
    "            if result_test is None:\n",
    "                result_test = f\n",
    "            else:\n",
    "                result_test = pd.concat([result_test, f], ignore_index=True, sort=False)\n",
    "        else: pass\n",
    "\n",
    "result_test[['acc ', 'auc_roc ', 'disparate_impact_favorable_06 ',\n",
    "       'disparate_impact_favorable_08 ', 'disparate_impact_unfavorable_06 ',\n",
    "       'disparate_impact_unfavorable_08 ', 'f1 ', 'fnr_priv_06 ',\n",
    "       'fnr_priv_08 ', 'fnr_total_06 ', 'fnr_total_08 ', 'fnr_unpriv_06 ',\n",
    "       'fnr_unpriv_08 ', 'fpr_priv_06 ', 'fpr_priv_08 ', 'fpr_total_06 ',\n",
    "       'fpr_total_08 ', 'fpr_unpriv_06 ', 'fpr_unpriv_08 ', 'loss ',\n",
    "       'precision ', 'priv_n_06 ', 'priv_n_08 ', 'priv_ratio_favorable_06 ',\n",
    "       'priv_ratio_favorable_08 ', 'priv_ratio_unfavorable_06 ',\n",
    "       'priv_ratio_unfavorable_08 ', 'priv_total_06 ', 'priv_total_08 ',\n",
    "       'recall ', 'unpriv_n_06 ', 'unpriv_n_08 ', 'unpriv_ratio_favorable_06 ',\n",
    "       'unpriv_ratio_favorable_08 ', 'unpriv_ratio_unfavorable_06 ',\n",
    "       'unpriv_ratio_unfavorable_08 ', 'unpriv_total_06 ', 'unpriv_total_08 ']] = result_test[['acc ', 'auc_roc ', 'disparate_impact_favorable_06 ',\n",
    "       'disparate_impact_favorable_08 ', 'disparate_impact_unfavorable_06 ',\n",
    "       'disparate_impact_unfavorable_08 ', 'f1 ', 'fnr_priv_06 ',\n",
    "       'fnr_priv_08 ', 'fnr_total_06 ', 'fnr_total_08 ', 'fnr_unpriv_06 ',\n",
    "       'fnr_unpriv_08 ', 'fpr_priv_06 ', 'fpr_priv_08 ', 'fpr_total_06 ',\n",
    "       'fpr_total_08 ', 'fpr_unpriv_06 ', 'fpr_unpriv_08 ', 'loss ',\n",
    "       'precision ', 'priv_n_06 ', 'priv_n_08 ', 'priv_ratio_favorable_06 ',\n",
    "       'priv_ratio_favorable_08 ', 'priv_ratio_unfavorable_06 ',\n",
    "       'priv_ratio_unfavorable_08 ', 'priv_total_06 ', 'priv_total_08 ',\n",
    "       'recall ', 'unpriv_n_06 ', 'unpriv_n_08 ', 'unpriv_ratio_favorable_06 ',\n",
    "       'unpriv_ratio_favorable_08 ', 'unpriv_ratio_unfavorable_06 ',\n",
    "       'unpriv_ratio_unfavorable_08 ', 'unpriv_total_06 ', 'unpriv_total_08 ',]].astype(float)\n",
    "\n",
    "result_val[['acc ', 'auc_roc ', 'disparate_impact_favorable_06 ',\n",
    "       'disparate_impact_favorable_08 ', 'disparate_impact_unfavorable_06 ',\n",
    "       'disparate_impact_unfavorable_08 ', 'f1 ', 'fnr_priv_06 ',\n",
    "       'fnr_priv_08 ', 'fnr_total_06 ', 'fnr_total_08 ', 'fnr_unpriv_06 ',\n",
    "       'fnr_unpriv_08 ', 'fpr_priv_06 ', 'fpr_priv_08 ', 'fpr_total_06 ',\n",
    "       'fpr_total_08 ', 'fpr_unpriv_06 ', 'fpr_unpriv_08 ', 'loss ',\n",
    "       'precision ', 'priv_n_06 ', 'priv_n_08 ', 'priv_ratio_favorable_06 ',\n",
    "       'priv_ratio_favorable_08 ', 'priv_ratio_unfavorable_06 ',\n",
    "       'priv_ratio_unfavorable_08 ', 'priv_total_06 ', 'priv_total_08 ',\n",
    "       'recall ', 'unpriv_n_06 ', 'unpriv_n_08 ', 'unpriv_ratio_favorable_06 ',\n",
    "       'unpriv_ratio_favorable_08 ', 'unpriv_ratio_unfavorable_06 ',\n",
    "       'unpriv_ratio_unfavorable_08 ', 'unpriv_total_06 ', 'unpriv_total_08 ']] = result_val[['acc ', 'auc_roc ', 'disparate_impact_favorable_06 ',\n",
    "       'disparate_impact_favorable_08 ', 'disparate_impact_unfavorable_06 ',\n",
    "       'disparate_impact_unfavorable_08 ', 'f1 ', 'fnr_priv_06 ',\n",
    "       'fnr_priv_08 ', 'fnr_total_06 ', 'fnr_total_08 ', 'fnr_unpriv_06 ',\n",
    "       'fnr_unpriv_08 ', 'fpr_priv_06 ', 'fpr_priv_08 ', 'fpr_total_06 ',\n",
    "       'fpr_total_08 ', 'fpr_unpriv_06 ', 'fpr_unpriv_08 ', 'loss ',\n",
    "       'precision ', 'priv_n_06 ', 'priv_n_08 ', 'priv_ratio_favorable_06 ',\n",
    "       'priv_ratio_favorable_08 ', 'priv_ratio_unfavorable_06 ',\n",
    "       'priv_ratio_unfavorable_08 ', 'priv_total_06 ', 'priv_total_08 ',\n",
    "       'recall ', 'unpriv_n_06 ', 'unpriv_n_08 ', 'unpriv_ratio_favorable_06 ',\n",
    "       'unpriv_ratio_favorable_08 ', 'unpriv_ratio_unfavorable_06 ',\n",
    "       'unpriv_ratio_unfavorable_08 ', 'unpriv_total_06 ', 'unpriv_total_08 ',]].astype(float)\n",
    "result_val.to_csv(f'{root}/log_reg_stats/val_results_raw.csv', index=False)\n",
    "result_test.to_csv(f'{root}/log_reg_stats/test_results_raw.csv', index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "0        acc   auc_roc  disparate_impact_favorable_06   \\\n0    0.879241   0.88808                       0.161209   \n1    0.876817    0.8932                       0.210575   \n2    0.878433  0.889583                        0.19658   \n3    0.880048  0.892417                       0.194653   \n4     0.88126  0.891925                       0.190506   \n..        ...       ...                            ...   \n145  0.937041  0.755284                        1.00671   \n146  0.937041  0.755591                        1.00706   \n147   0.93696  0.755047                        1.00662   \n148  0.937041  0.755537                        1.00758   \n149   0.93688    0.7552                        1.00671   \n\n0   disparate_impact_favorable_08  disparate_impact_unfavorable_06   \\\n0                         0.121588                          1.18623   \n1                         0.109897                          1.19575   \n2                         0.217183                          1.20363   \n3                         0.215139                          1.20666   \n4                                0                          1.18581   \n..                             ...                              ...   \n145                        1.01341                         0.508529   \n146                        1.01374                          0.49605   \n147                        1.01332                         0.511748   \n148                        1.01424                         0.478439   \n149                        1.01341                         0.508529   \n\n0   disparate_impact_unfavorable_08        f1  fnr_priv_06  fnr_priv_08   \\\n0                            1.16135  0.927725    0.0692943    0.0594458   \n1                             1.1845  0.925555    0.0813732    0.0710327   \n2                            1.16462  0.926424    0.0826446    0.0710327   \n3                            1.16695  0.927331    0.0820089    0.0710327   \n4                            1.19183   0.92871    0.0705658    0.0619647   \n..                               ...       ...          ...          ...   \n145                                0  0.245648     0.848558     0.853009   \n146                                0  0.248555     0.846154     0.850694   \n147                                0   0.24395      0.84976     0.854167   \n148                                0  0.252874     0.842548     0.847222   \n149                                0  0.243714      0.84976     0.854167   \n\n0   fnr_total_06   ... batch_size epochs max_seq_length learning_rate  \\\n0       0.0583906  ...         32   1000            128         0.002   \n1       0.0696762  ...         32   1000            128         0.002   \n2       0.0701668  ...         32   1000            128         0.002   \n3       0.0701668  ...         32   1000            128         0.002   \n4       0.0603533  ...         32   1000            128         0.002   \n..            ...  ...        ...    ...            ...           ...   \n145      0.854191  ...         64   1000            128         0.002   \n146      0.851894  ...         64   1000            128         0.002   \n147      0.855339  ...         64   1000            128         0.002   \n148       0.84845  ...         64   1000            128         0.002   \n149      0.855339  ...         64   1000            128         0.002   \n\n0   reg_strength class_weight  \\\n0              0         none   \n1              0         none   \n2              0         none   \n3              0         none   \n4              0         none   \n..           ...          ...   \n145            0         none   \n146            0         none   \n147            0         none   \n148            0         none   \n149            0         none   \n\n0                                           output_dir do_train do_eval  test  \n0    /nethome/mhalevy3/HateSpeech/benchmarking/base...    False   False  True  \n1    /nethome/mhalevy3/HateSpeech/benchmarking/base...    False   False  True  \n2    /nethome/mhalevy3/HateSpeech/benchmarking/base...    False   False  True  \n3    /nethome/mhalevy3/HateSpeech/benchmarking/base...    False   False  True  \n4    /nethome/mhalevy3/HateSpeech/benchmarking/base...    False   False  True  \n..                                                 ...      ...     ...   ...  \n145  /nethome/mhalevy3/HateSpeech/benchmarking/base...    False   False  True  \n146  /nethome/mhalevy3/HateSpeech/benchmarking/base...    False   False  True  \n147  /nethome/mhalevy3/HateSpeech/benchmarking/base...    False   False  True  \n148  /nethome/mhalevy3/HateSpeech/benchmarking/base...    False   False  True  \n149  /nethome/mhalevy3/HateSpeech/benchmarking/base...    False   False  True  \n\n[150 rows x 55 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>acc</th>\n      <th>auc_roc</th>\n      <th>disparate_impact_favorable_06</th>\n      <th>disparate_impact_favorable_08</th>\n      <th>disparate_impact_unfavorable_06</th>\n      <th>disparate_impact_unfavorable_08</th>\n      <th>f1</th>\n      <th>fnr_priv_06</th>\n      <th>fnr_priv_08</th>\n      <th>fnr_total_06</th>\n      <th>...</th>\n      <th>batch_size</th>\n      <th>epochs</th>\n      <th>max_seq_length</th>\n      <th>learning_rate</th>\n      <th>reg_strength</th>\n      <th>class_weight</th>\n      <th>output_dir</th>\n      <th>do_train</th>\n      <th>do_eval</th>\n      <th>test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.879241</td>\n      <td>0.88808</td>\n      <td>0.161209</td>\n      <td>0.121588</td>\n      <td>1.18623</td>\n      <td>1.16135</td>\n      <td>0.927725</td>\n      <td>0.0692943</td>\n      <td>0.0594458</td>\n      <td>0.0583906</td>\n      <td>...</td>\n      <td>32</td>\n      <td>1000</td>\n      <td>128</td>\n      <td>0.002</td>\n      <td>0</td>\n      <td>none</td>\n      <td>/nethome/mhalevy3/HateSpeech/benchmarking/base...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.876817</td>\n      <td>0.8932</td>\n      <td>0.210575</td>\n      <td>0.109897</td>\n      <td>1.19575</td>\n      <td>1.1845</td>\n      <td>0.925555</td>\n      <td>0.0813732</td>\n      <td>0.0710327</td>\n      <td>0.0696762</td>\n      <td>...</td>\n      <td>32</td>\n      <td>1000</td>\n      <td>128</td>\n      <td>0.002</td>\n      <td>0</td>\n      <td>none</td>\n      <td>/nethome/mhalevy3/HateSpeech/benchmarking/base...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.878433</td>\n      <td>0.889583</td>\n      <td>0.19658</td>\n      <td>0.217183</td>\n      <td>1.20363</td>\n      <td>1.16462</td>\n      <td>0.926424</td>\n      <td>0.0826446</td>\n      <td>0.0710327</td>\n      <td>0.0701668</td>\n      <td>...</td>\n      <td>32</td>\n      <td>1000</td>\n      <td>128</td>\n      <td>0.002</td>\n      <td>0</td>\n      <td>none</td>\n      <td>/nethome/mhalevy3/HateSpeech/benchmarking/base...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.880048</td>\n      <td>0.892417</td>\n      <td>0.194653</td>\n      <td>0.215139</td>\n      <td>1.20666</td>\n      <td>1.16695</td>\n      <td>0.927331</td>\n      <td>0.0820089</td>\n      <td>0.0710327</td>\n      <td>0.0701668</td>\n      <td>...</td>\n      <td>32</td>\n      <td>1000</td>\n      <td>128</td>\n      <td>0.002</td>\n      <td>0</td>\n      <td>none</td>\n      <td>/nethome/mhalevy3/HateSpeech/benchmarking/base...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.88126</td>\n      <td>0.891925</td>\n      <td>0.190506</td>\n      <td>0</td>\n      <td>1.18581</td>\n      <td>1.19183</td>\n      <td>0.92871</td>\n      <td>0.0705658</td>\n      <td>0.0619647</td>\n      <td>0.0603533</td>\n      <td>...</td>\n      <td>32</td>\n      <td>1000</td>\n      <td>128</td>\n      <td>0.002</td>\n      <td>0</td>\n      <td>none</td>\n      <td>/nethome/mhalevy3/HateSpeech/benchmarking/base...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>0.937041</td>\n      <td>0.755284</td>\n      <td>1.00671</td>\n      <td>1.01341</td>\n      <td>0.508529</td>\n      <td>0</td>\n      <td>0.245648</td>\n      <td>0.848558</td>\n      <td>0.853009</td>\n      <td>0.854191</td>\n      <td>...</td>\n      <td>64</td>\n      <td>1000</td>\n      <td>128</td>\n      <td>0.002</td>\n      <td>0</td>\n      <td>none</td>\n      <td>/nethome/mhalevy3/HateSpeech/benchmarking/base...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>0.937041</td>\n      <td>0.755591</td>\n      <td>1.00706</td>\n      <td>1.01374</td>\n      <td>0.49605</td>\n      <td>0</td>\n      <td>0.248555</td>\n      <td>0.846154</td>\n      <td>0.850694</td>\n      <td>0.851894</td>\n      <td>...</td>\n      <td>64</td>\n      <td>1000</td>\n      <td>128</td>\n      <td>0.002</td>\n      <td>0</td>\n      <td>none</td>\n      <td>/nethome/mhalevy3/HateSpeech/benchmarking/base...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>0.93696</td>\n      <td>0.755047</td>\n      <td>1.00662</td>\n      <td>1.01332</td>\n      <td>0.511748</td>\n      <td>0</td>\n      <td>0.24395</td>\n      <td>0.84976</td>\n      <td>0.854167</td>\n      <td>0.855339</td>\n      <td>...</td>\n      <td>64</td>\n      <td>1000</td>\n      <td>128</td>\n      <td>0.002</td>\n      <td>0</td>\n      <td>none</td>\n      <td>/nethome/mhalevy3/HateSpeech/benchmarking/base...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>0.937041</td>\n      <td>0.755537</td>\n      <td>1.00758</td>\n      <td>1.01424</td>\n      <td>0.478439</td>\n      <td>0</td>\n      <td>0.252874</td>\n      <td>0.842548</td>\n      <td>0.847222</td>\n      <td>0.84845</td>\n      <td>...</td>\n      <td>64</td>\n      <td>1000</td>\n      <td>128</td>\n      <td>0.002</td>\n      <td>0</td>\n      <td>none</td>\n      <td>/nethome/mhalevy3/HateSpeech/benchmarking/base...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>0.93688</td>\n      <td>0.7552</td>\n      <td>1.00671</td>\n      <td>1.01341</td>\n      <td>0.508529</td>\n      <td>0</td>\n      <td>0.243714</td>\n      <td>0.84976</td>\n      <td>0.854167</td>\n      <td>0.855339</td>\n      <td>...</td>\n      <td>64</td>\n      <td>1000</td>\n      <td>128</td>\n      <td>0.002</td>\n      <td>0</td>\n      <td>none</td>\n      <td>/nethome/mhalevy3/HateSpeech/benchmarking/base...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows × 55 columns</p>\n</div>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_val"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "0        acc   auc_roc  disparate_impact_favorable_06   \\\n0    0.886235   0.90999                       0.160013   \n1    0.887045  0.908225                       0.147937   \n2    0.887854   0.90499                       0.173556   \n3    0.889474  0.906283                       0.174464   \n4    0.888664  0.906275                       0.142358   \n..        ...       ...                            ...   \n145  0.936557  0.722813                       0.985477   \n146  0.936234  0.723528                       0.985815   \n147  0.936557  0.723133                       0.985477   \n148  0.936395  0.723617                       0.985984   \n149  0.936476  0.722577                       0.985393   \n\n0   disparate_impact_favorable_08  disparate_impact_unfavorable_06   \\\n0                                0                           1.1773   \n1                                0                          1.19794   \n2                                0                          1.20035   \n3                                0                          1.19884   \n4                                0                          1.19071   \n..                             ...                              ...   \n145                       0.982049                          2.19436   \n146                       0.982372                          2.13424   \n147                       0.982049                          2.19436   \n148                       0.982534                           2.1054   \n149                       0.981969                          2.20992   \n\n0   disparate_impact_unfavorable_08        f1  fnr_priv_06  fnr_priv_08   \\\n0                            1.17509  0.932468    0.0647851    0.0547112   \n1                            1.19145  0.932495    0.0731238    0.0612969   \n2                              1.201  0.932718    0.0769724     0.064843   \n3                             1.1998  0.933722    0.0744067    0.0633232   \n4                            1.18321  0.933687    0.0679923    0.0567376   \n..                               ...       ...          ...          ...   \n145                          2.40919  0.235409     0.864897     0.861432   \n146                          2.34858  0.234496     0.864897     0.861432   \n147                          2.40919  0.235409     0.864897     0.861432   \n148                          2.31941  0.237911     0.862485     0.859122   \n149                          2.42483   0.23369     0.866104     0.862587   \n\n0   fnr_total_06   ... batch_size epochs max_seq_length learning_rate  \\\n0       0.0527344  ...         32   1000            128         0.002   \n1        0.059082  ...         32   1000            128         0.002   \n2          0.0625  ...         32   1000            128         0.002   \n3       0.0610352  ...         32   1000            128         0.002   \n4       0.0546875  ...         32   1000            128         0.002   \n..            ...  ...        ...    ...            ...           ...   \n145      0.861079  ...         64   1000            128         0.002   \n146      0.861079  ...         64   1000            128         0.002   \n147      0.861079  ...         64   1000            128         0.002   \n148      0.858783  ...         64   1000            128         0.002   \n149      0.862227  ...         64   1000            128         0.002   \n\n0   reg_strength class_weight  \\\n0              0         none   \n1              0         none   \n2              0         none   \n3              0         none   \n4              0         none   \n..           ...          ...   \n145            0         none   \n146            0         none   \n147            0         none   \n148            0         none   \n149            0         none   \n\n0                                           output_dir do_train do_eval  test  \n0    /nethome/mhalevy3/HateSpeech/benchmarking/base...    False   False  True  \n1    /nethome/mhalevy3/HateSpeech/benchmarking/base...    False   False  True  \n2    /nethome/mhalevy3/HateSpeech/benchmarking/base...    False   False  True  \n3    /nethome/mhalevy3/HateSpeech/benchmarking/base...    False   False  True  \n4    /nethome/mhalevy3/HateSpeech/benchmarking/base...    False   False  True  \n..                                                 ...      ...     ...   ...  \n145  /nethome/mhalevy3/HateSpeech/benchmarking/base...    False   False  True  \n146  /nethome/mhalevy3/HateSpeech/benchmarking/base...    False   False  True  \n147  /nethome/mhalevy3/HateSpeech/benchmarking/base...    False   False  True  \n148  /nethome/mhalevy3/HateSpeech/benchmarking/base...    False   False  True  \n149  /nethome/mhalevy3/HateSpeech/benchmarking/base...    False   False  True  \n\n[150 rows x 55 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>acc</th>\n      <th>auc_roc</th>\n      <th>disparate_impact_favorable_06</th>\n      <th>disparate_impact_favorable_08</th>\n      <th>disparate_impact_unfavorable_06</th>\n      <th>disparate_impact_unfavorable_08</th>\n      <th>f1</th>\n      <th>fnr_priv_06</th>\n      <th>fnr_priv_08</th>\n      <th>fnr_total_06</th>\n      <th>...</th>\n      <th>batch_size</th>\n      <th>epochs</th>\n      <th>max_seq_length</th>\n      <th>learning_rate</th>\n      <th>reg_strength</th>\n      <th>class_weight</th>\n      <th>output_dir</th>\n      <th>do_train</th>\n      <th>do_eval</th>\n      <th>test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.886235</td>\n      <td>0.90999</td>\n      <td>0.160013</td>\n      <td>0</td>\n      <td>1.1773</td>\n      <td>1.17509</td>\n      <td>0.932468</td>\n      <td>0.0647851</td>\n      <td>0.0547112</td>\n      <td>0.0527344</td>\n      <td>...</td>\n      <td>32</td>\n      <td>1000</td>\n      <td>128</td>\n      <td>0.002</td>\n      <td>0</td>\n      <td>none</td>\n      <td>/nethome/mhalevy3/HateSpeech/benchmarking/base...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.887045</td>\n      <td>0.908225</td>\n      <td>0.147937</td>\n      <td>0</td>\n      <td>1.19794</td>\n      <td>1.19145</td>\n      <td>0.932495</td>\n      <td>0.0731238</td>\n      <td>0.0612969</td>\n      <td>0.059082</td>\n      <td>...</td>\n      <td>32</td>\n      <td>1000</td>\n      <td>128</td>\n      <td>0.002</td>\n      <td>0</td>\n      <td>none</td>\n      <td>/nethome/mhalevy3/HateSpeech/benchmarking/base...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.887854</td>\n      <td>0.90499</td>\n      <td>0.173556</td>\n      <td>0</td>\n      <td>1.20035</td>\n      <td>1.201</td>\n      <td>0.932718</td>\n      <td>0.0769724</td>\n      <td>0.064843</td>\n      <td>0.0625</td>\n      <td>...</td>\n      <td>32</td>\n      <td>1000</td>\n      <td>128</td>\n      <td>0.002</td>\n      <td>0</td>\n      <td>none</td>\n      <td>/nethome/mhalevy3/HateSpeech/benchmarking/base...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.889474</td>\n      <td>0.906283</td>\n      <td>0.174464</td>\n      <td>0</td>\n      <td>1.19884</td>\n      <td>1.1998</td>\n      <td>0.933722</td>\n      <td>0.0744067</td>\n      <td>0.0633232</td>\n      <td>0.0610352</td>\n      <td>...</td>\n      <td>32</td>\n      <td>1000</td>\n      <td>128</td>\n      <td>0.002</td>\n      <td>0</td>\n      <td>none</td>\n      <td>/nethome/mhalevy3/HateSpeech/benchmarking/base...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.888664</td>\n      <td>0.906275</td>\n      <td>0.142358</td>\n      <td>0</td>\n      <td>1.19071</td>\n      <td>1.18321</td>\n      <td>0.933687</td>\n      <td>0.0679923</td>\n      <td>0.0567376</td>\n      <td>0.0546875</td>\n      <td>...</td>\n      <td>32</td>\n      <td>1000</td>\n      <td>128</td>\n      <td>0.002</td>\n      <td>0</td>\n      <td>none</td>\n      <td>/nethome/mhalevy3/HateSpeech/benchmarking/base...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>0.936557</td>\n      <td>0.722813</td>\n      <td>0.985477</td>\n      <td>0.982049</td>\n      <td>2.19436</td>\n      <td>2.40919</td>\n      <td>0.235409</td>\n      <td>0.864897</td>\n      <td>0.861432</td>\n      <td>0.861079</td>\n      <td>...</td>\n      <td>64</td>\n      <td>1000</td>\n      <td>128</td>\n      <td>0.002</td>\n      <td>0</td>\n      <td>none</td>\n      <td>/nethome/mhalevy3/HateSpeech/benchmarking/base...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>0.936234</td>\n      <td>0.723528</td>\n      <td>0.985815</td>\n      <td>0.982372</td>\n      <td>2.13424</td>\n      <td>2.34858</td>\n      <td>0.234496</td>\n      <td>0.864897</td>\n      <td>0.861432</td>\n      <td>0.861079</td>\n      <td>...</td>\n      <td>64</td>\n      <td>1000</td>\n      <td>128</td>\n      <td>0.002</td>\n      <td>0</td>\n      <td>none</td>\n      <td>/nethome/mhalevy3/HateSpeech/benchmarking/base...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>0.936557</td>\n      <td>0.723133</td>\n      <td>0.985477</td>\n      <td>0.982049</td>\n      <td>2.19436</td>\n      <td>2.40919</td>\n      <td>0.235409</td>\n      <td>0.864897</td>\n      <td>0.861432</td>\n      <td>0.861079</td>\n      <td>...</td>\n      <td>64</td>\n      <td>1000</td>\n      <td>128</td>\n      <td>0.002</td>\n      <td>0</td>\n      <td>none</td>\n      <td>/nethome/mhalevy3/HateSpeech/benchmarking/base...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>0.936395</td>\n      <td>0.723617</td>\n      <td>0.985984</td>\n      <td>0.982534</td>\n      <td>2.1054</td>\n      <td>2.31941</td>\n      <td>0.237911</td>\n      <td>0.862485</td>\n      <td>0.859122</td>\n      <td>0.858783</td>\n      <td>...</td>\n      <td>64</td>\n      <td>1000</td>\n      <td>128</td>\n      <td>0.002</td>\n      <td>0</td>\n      <td>none</td>\n      <td>/nethome/mhalevy3/HateSpeech/benchmarking/base...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>0.936476</td>\n      <td>0.722577</td>\n      <td>0.985393</td>\n      <td>0.981969</td>\n      <td>2.20992</td>\n      <td>2.42483</td>\n      <td>0.23369</td>\n      <td>0.866104</td>\n      <td>0.862587</td>\n      <td>0.862227</td>\n      <td>...</td>\n      <td>64</td>\n      <td>1000</td>\n      <td>128</td>\n      <td>0.002</td>\n      <td>0</td>\n      <td>none</td>\n      <td>/nethome/mhalevy3/HateSpeech/benchmarking/base...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows × 55 columns</p>\n</div>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['acc ', 'auc_roc ', 'disparate_impact_favorable_06 ',\n       'disparate_impact_favorable_08 ', 'disparate_impact_unfavorable_06 ',\n       'disparate_impact_unfavorable_08 ', 'f1 ', 'fnr_priv_06 ',\n       'fnr_priv_08 ', 'fnr_total_06 ', 'fnr_total_08 ', 'fnr_unpriv_06 ',\n       'fnr_unpriv_08 ', 'fpr_priv_06 ', 'fpr_priv_08 ', 'fpr_total_06 ',\n       'fpr_total_08 ', 'fpr_unpriv_06 ', 'fpr_unpriv_08 ', 'loss ',\n       'precision ', 'priv_n_06 ', 'priv_n_08 ', 'priv_ratio_favorable_06 ',\n       'priv_ratio_favorable_08 ', 'priv_ratio_unfavorable_06 ',\n       'priv_ratio_unfavorable_08 ', 'priv_total_06 ', 'priv_total_08 ',\n       'recall ', 'unpriv_n_06 ', 'unpriv_n_08 ', 'unpriv_ratio_favorable_06 ',\n       'unpriv_ratio_favorable_08 ', 'unpriv_ratio_unfavorable_06 ',\n       'unpriv_ratio_unfavorable_08 ', 'unpriv_total_06 ', 'unpriv_total_08 ',\n       'is_local', 'is_gcloud', 'is_gridsearch', 'retrain', 'task_name',\n       'seed', 'dataset', 'batch_size', 'epochs', 'max_seq_length',\n       'learning_rate', 'reg_strength', 'class_weight', 'output_dir',\n       'do_train', 'do_eval', 'test'],\n      dtype='object', name=0)"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_val.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def ci_sample(series):\n",
    "    pop_mean = series.mean()\n",
    "    pop_std = series.std(ddof=0)\n",
    "    pop_n =  len(series) - 1\n",
    "    return pop_mean - (1.96*pop_std/math.sqrt(pop_n)), pop_mean + (1.96*pop_std/math.sqrt(pop_n))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "0\nacc                                 object\nauc_roc                             object\ndisparate_impact_favorable_06       object\ndisparate_impact_favorable_08       object\ndisparate_impact_unfavorable_06     object\ndisparate_impact_unfavorable_08     object\nf1                                  object\nfnr_priv_06                         object\nfnr_priv_08                         object\nfnr_total_06                        object\nfnr_total_08                        object\nfnr_unpriv_06                       object\nfnr_unpriv_08                       object\nfpr_priv_06                         object\nfpr_priv_08                         object\nfpr_total_06                        object\nfpr_total_08                        object\nfpr_unpriv_06                       object\nfpr_unpriv_08                       object\nloss                                object\nprecision                           object\npriv_n_06                           object\npriv_n_08                           object\npriv_ratio_favorable_06             object\npriv_ratio_favorable_08             object\npriv_ratio_unfavorable_06           object\npriv_ratio_unfavorable_08           object\npriv_total_06                       object\npriv_total_08                       object\nrecall                              object\nunpriv_n_06                         object\nunpriv_n_08                         object\nunpriv_ratio_favorable_06           object\nunpriv_ratio_favorable_08           object\nunpriv_ratio_unfavorable_06         object\nunpriv_ratio_unfavorable_08         object\nunpriv_total_06                     object\nunpriv_total_08                     object\nis_local                            object\nis_gcloud                           object\nis_gridsearch                       object\nretrain                             object\ntask_name                           object\nseed                                object\ndataset                             object\nbatch_size                          object\nepochs                              object\nmax_seq_length                      object\nlearning_rate                       object\nreg_strength                        object\nclass_weight                        object\noutput_dir                          object\ndo_train                            object\ndo_eval                             object\ntest                                object\ndtype: object"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_val.dtypes\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "       dataset task_name  acc_mean   acc_std  \\\n0     davidson     glove  0.879564  0.001636   \n1     davidson     ngram  0.942569  0.001422   \n2     davidson    tf_idf  0.875121  0.000847   \n3       founta     glove  0.897611  0.000834   \n4       founta     ngram  0.928777  0.000284   \n5       founta    tf_idf  0.893553  0.000328   \n6      golbeck     glove  0.759959  0.001538   \n7      golbeck     ngram  0.774730  0.001031   \n8      golbeck    tf_idf  0.763973  0.000497   \n9   harassment     glove  0.834568  0.000679   \n10  harassment     ngram  0.891841  0.000374   \n11  harassment    tf_idf  0.841425  0.000436   \n12        hate     glove  0.932036  0.000219   \n13        hate     ngram  0.939898  0.000175   \n14        hate    tf_idf  0.936976  0.000074   \n\n                                      acc_ci  auc_roc_mean  auc_roc_std  \\\n0   (0.8785500579036941, 0.8805775470577012)      0.890331     0.002222   \n1   (0.9416871656077274, 0.9434501420361631)      0.975600     0.000472   \n2   (0.8745960717061076, 0.8756462569376913)      0.910278     0.000839   \n3   (0.8970938947205397, 0.8981280777449755)      0.932995     0.000249   \n4   (0.9286012626306578, 0.9289530181272462)      0.955304     0.000134   \n5   (0.8933493719192672, 0.8937565579322647)      0.914120     0.000489   \n6     (0.759005394838842, 0.760912260152308)      0.589601     0.005645   \n7   (0.7740909087029919, 0.7753686679037586)      0.653591     0.000959   \n8   (0.7636650472457931, 0.7642814249221757)      0.604567     0.000637   \n9   (0.8341469976975502, 0.8349888231680809)      0.899041     0.000188   \n10   (0.8916093782077561, 0.892073504578041)      0.940094     0.000089   \n11  (0.8411545901034245, 0.8416953177716364)      0.887627     0.000099   \n12  (0.9319009918665166, 0.9321719508672479)      0.747700     0.000700   \n13  (0.9397897014513805, 0.9400068750485584)      0.845559     0.000209   \n14  (0.9369303948864447, 0.9370223276629939)      0.755414     0.000466   \n\n                                  auc_roc_ci   f1_mean    f1_std  ...  \\\n0    (0.8889534336637371, 0.891708173124898)  0.927512  0.001172  ...   \n1   (0.9753069607280388, 0.9758921178318366)  0.965446  0.000813  ...   \n2    (0.9097577729885638, 0.910797615938896)  0.927679  0.000477  ...   \n3   (0.9328408853341235, 0.9331496341894971)  0.806685  0.002951  ...   \n4   (0.9552211418682848, 0.9553866490786758)  0.866708  0.000676  ...   \n5    (0.9138168199037415, 0.914422430279364)  0.786157  0.000982  ...   \n6   (0.5861018409905066, 0.5930997977080714)  0.052724  0.011394  ...   \n7   (0.6529971964843347, 0.6541857354156896)  0.214033  0.004304  ...   \n8   (0.6041717845499798, 0.6049612682759479)  0.180735  0.006150  ...   \n9   (0.8989244358581123, 0.8991577370125237)  0.782852  0.001256  ...   \n10  (0.9400392674556989, 0.9401496983417253)  0.857027  0.000636  ...   \n11  (0.8875659028939921, 0.8876888950414937)  0.776939  0.001277  ...   \n12    (0.7472665422467533, 0.74813399381358)  0.181555  0.004138  ...   \n13  (0.8454292276271003, 0.8456877848736626)  0.342456  0.002477  ...   \n14  (0.7551246294842189, 0.7557026138438756)  0.246324  0.002781  ...   \n\n   fpr_unpriv_06_mean  fpr_unpriv_06_std  \\\n0            0.607692           0.076495   \n1            0.384615           0.000000   \n2            0.846154           0.000000   \n3            0.500000           0.000000   \n4            0.337500           0.013176   \n5            0.300000           0.032867   \n6            0.000000           0.000000   \n7            0.000000           0.000000   \n8            0.000000           0.000000   \n9            0.572093           0.012009   \n10           0.418605           0.000000   \n11           0.509302           0.007354   \n12           0.003670           0.000000   \n13           0.007339           0.000000   \n14           0.005505           0.000000   \n\n                                  fpr_unpriv_06_ci fpr_unpriv_08_mean  \\\n0         (0.5602805050799206, 0.6551041103046947)                0.0   \n1       (0.38461538461538464, 0.38461538461538464)                0.0   \n2         (0.8461538461538461, 0.8461538461538461)                0.0   \n3                                       (0.5, 0.5)                0.0   \n4        (0.32933333333333337, 0.3456666666666667)                0.0   \n5        (0.27962875422756417, 0.3203712457724359)                0.0   \n6                                       (0.0, 0.0)                0.0   \n7                                       (0.0, 0.0)                0.0   \n8                                       (0.0, 0.0)                0.0   \n9         (0.5646496125645659, 0.5795364339470621)                0.6   \n10        (0.4186046511627907, 0.4186046511627907)                0.8   \n11         (0.5047441860465116, 0.513860465116279)                0.8   \n12  (0.0036697247706422025, 0.0036697247706422033)                0.0   \n13    (0.007339449541284405, 0.007339449541284407)                0.0   \n14    (0.005504587155963303, 0.005504587155963303)                0.0   \n\n    fpr_unpriv_08_std           fpr_unpriv_08_ci priv_n_06  priv_n_08  \\\n0                 0.0                 (0.0, 0.0)     478.0       53.0   \n1                 0.0                 (0.0, 0.0)     478.0       53.0   \n2                 0.0                 (0.0, 0.0)     478.0       53.0   \n3                 0.0                 (0.0, 0.0)     139.0       10.0   \n4                 0.0                 (0.0, 0.0)     139.0       10.0   \n5                 0.0                 (0.0, 0.0)     139.0       10.0   \n6                 0.0                 (0.0, 0.0)       8.0        1.0   \n7                 0.0                 (0.0, 0.0)       8.0        1.0   \n8                 0.0                 (0.0, 0.0)       8.0        1.0   \n9                 0.0  (0.5999999999999998, 0.6)     648.0       79.0   \n10                0.0                 (0.8, 0.8)     648.0       79.0   \n11                0.0                 (0.8, 0.8)     648.0       79.0   \n12                0.0                 (0.0, 0.0)     584.0       67.0   \n13                0.0                 (0.0, 0.0)     584.0       67.0   \n14                0.0                 (0.0, 0.0)     584.0       67.0   \n\n    unpriv_n_06 unpriv_n_08  \n0        1998.0      2423.0  \n1        1998.0      2423.0  \n2        1998.0      2423.0  \n3        9028.0      9157.0  \n4        9028.0      9157.0  \n5        9028.0      9157.0  \n6        1935.0      1942.0  \n7        1935.0      1942.0  \n8        1935.0      1942.0  \n9       12798.0     13367.0  \n10      12798.0     13367.0  \n11      12798.0     13367.0  \n12      11805.0     12322.0  \n13      11805.0     12322.0  \n14      11805.0     12322.0  \n\n[15 rows x 69 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dataset</th>\n      <th>task_name</th>\n      <th>acc_mean</th>\n      <th>acc_std</th>\n      <th>acc_ci</th>\n      <th>auc_roc_mean</th>\n      <th>auc_roc_std</th>\n      <th>auc_roc_ci</th>\n      <th>f1_mean</th>\n      <th>f1_std</th>\n      <th>...</th>\n      <th>fpr_unpriv_06_mean</th>\n      <th>fpr_unpriv_06_std</th>\n      <th>fpr_unpriv_06_ci</th>\n      <th>fpr_unpriv_08_mean</th>\n      <th>fpr_unpriv_08_std</th>\n      <th>fpr_unpriv_08_ci</th>\n      <th>priv_n_06</th>\n      <th>priv_n_08</th>\n      <th>unpriv_n_06</th>\n      <th>unpriv_n_08</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>davidson</td>\n      <td>glove</td>\n      <td>0.879564</td>\n      <td>0.001636</td>\n      <td>(0.8785500579036941, 0.8805775470577012)</td>\n      <td>0.890331</td>\n      <td>0.002222</td>\n      <td>(0.8889534336637371, 0.891708173124898)</td>\n      <td>0.927512</td>\n      <td>0.001172</td>\n      <td>...</td>\n      <td>0.607692</td>\n      <td>0.076495</td>\n      <td>(0.5602805050799206, 0.6551041103046947)</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>(0.0, 0.0)</td>\n      <td>478.0</td>\n      <td>53.0</td>\n      <td>1998.0</td>\n      <td>2423.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>davidson</td>\n      <td>ngram</td>\n      <td>0.942569</td>\n      <td>0.001422</td>\n      <td>(0.9416871656077274, 0.9434501420361631)</td>\n      <td>0.975600</td>\n      <td>0.000472</td>\n      <td>(0.9753069607280388, 0.9758921178318366)</td>\n      <td>0.965446</td>\n      <td>0.000813</td>\n      <td>...</td>\n      <td>0.384615</td>\n      <td>0.000000</td>\n      <td>(0.38461538461538464, 0.38461538461538464)</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>(0.0, 0.0)</td>\n      <td>478.0</td>\n      <td>53.0</td>\n      <td>1998.0</td>\n      <td>2423.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>davidson</td>\n      <td>tf_idf</td>\n      <td>0.875121</td>\n      <td>0.000847</td>\n      <td>(0.8745960717061076, 0.8756462569376913)</td>\n      <td>0.910278</td>\n      <td>0.000839</td>\n      <td>(0.9097577729885638, 0.910797615938896)</td>\n      <td>0.927679</td>\n      <td>0.000477</td>\n      <td>...</td>\n      <td>0.846154</td>\n      <td>0.000000</td>\n      <td>(0.8461538461538461, 0.8461538461538461)</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>(0.0, 0.0)</td>\n      <td>478.0</td>\n      <td>53.0</td>\n      <td>1998.0</td>\n      <td>2423.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>founta</td>\n      <td>glove</td>\n      <td>0.897611</td>\n      <td>0.000834</td>\n      <td>(0.8970938947205397, 0.8981280777449755)</td>\n      <td>0.932995</td>\n      <td>0.000249</td>\n      <td>(0.9328408853341235, 0.9331496341894971)</td>\n      <td>0.806685</td>\n      <td>0.002951</td>\n      <td>...</td>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>(0.5, 0.5)</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>(0.0, 0.0)</td>\n      <td>139.0</td>\n      <td>10.0</td>\n      <td>9028.0</td>\n      <td>9157.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>founta</td>\n      <td>ngram</td>\n      <td>0.928777</td>\n      <td>0.000284</td>\n      <td>(0.9286012626306578, 0.9289530181272462)</td>\n      <td>0.955304</td>\n      <td>0.000134</td>\n      <td>(0.9552211418682848, 0.9553866490786758)</td>\n      <td>0.866708</td>\n      <td>0.000676</td>\n      <td>...</td>\n      <td>0.337500</td>\n      <td>0.013176</td>\n      <td>(0.32933333333333337, 0.3456666666666667)</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>(0.0, 0.0)</td>\n      <td>139.0</td>\n      <td>10.0</td>\n      <td>9028.0</td>\n      <td>9157.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>founta</td>\n      <td>tf_idf</td>\n      <td>0.893553</td>\n      <td>0.000328</td>\n      <td>(0.8933493719192672, 0.8937565579322647)</td>\n      <td>0.914120</td>\n      <td>0.000489</td>\n      <td>(0.9138168199037415, 0.914422430279364)</td>\n      <td>0.786157</td>\n      <td>0.000982</td>\n      <td>...</td>\n      <td>0.300000</td>\n      <td>0.032867</td>\n      <td>(0.27962875422756417, 0.3203712457724359)</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>(0.0, 0.0)</td>\n      <td>139.0</td>\n      <td>10.0</td>\n      <td>9028.0</td>\n      <td>9157.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>golbeck</td>\n      <td>glove</td>\n      <td>0.759959</td>\n      <td>0.001538</td>\n      <td>(0.759005394838842, 0.760912260152308)</td>\n      <td>0.589601</td>\n      <td>0.005645</td>\n      <td>(0.5861018409905066, 0.5930997977080714)</td>\n      <td>0.052724</td>\n      <td>0.011394</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>(0.0, 0.0)</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>(0.0, 0.0)</td>\n      <td>8.0</td>\n      <td>1.0</td>\n      <td>1935.0</td>\n      <td>1942.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>golbeck</td>\n      <td>ngram</td>\n      <td>0.774730</td>\n      <td>0.001031</td>\n      <td>(0.7740909087029919, 0.7753686679037586)</td>\n      <td>0.653591</td>\n      <td>0.000959</td>\n      <td>(0.6529971964843347, 0.6541857354156896)</td>\n      <td>0.214033</td>\n      <td>0.004304</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>(0.0, 0.0)</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>(0.0, 0.0)</td>\n      <td>8.0</td>\n      <td>1.0</td>\n      <td>1935.0</td>\n      <td>1942.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>golbeck</td>\n      <td>tf_idf</td>\n      <td>0.763973</td>\n      <td>0.000497</td>\n      <td>(0.7636650472457931, 0.7642814249221757)</td>\n      <td>0.604567</td>\n      <td>0.000637</td>\n      <td>(0.6041717845499798, 0.6049612682759479)</td>\n      <td>0.180735</td>\n      <td>0.006150</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>(0.0, 0.0)</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>(0.0, 0.0)</td>\n      <td>8.0</td>\n      <td>1.0</td>\n      <td>1935.0</td>\n      <td>1942.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>harassment</td>\n      <td>glove</td>\n      <td>0.834568</td>\n      <td>0.000679</td>\n      <td>(0.8341469976975502, 0.8349888231680809)</td>\n      <td>0.899041</td>\n      <td>0.000188</td>\n      <td>(0.8989244358581123, 0.8991577370125237)</td>\n      <td>0.782852</td>\n      <td>0.001256</td>\n      <td>...</td>\n      <td>0.572093</td>\n      <td>0.012009</td>\n      <td>(0.5646496125645659, 0.5795364339470621)</td>\n      <td>0.6</td>\n      <td>0.0</td>\n      <td>(0.5999999999999998, 0.6)</td>\n      <td>648.0</td>\n      <td>79.0</td>\n      <td>12798.0</td>\n      <td>13367.0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>harassment</td>\n      <td>ngram</td>\n      <td>0.891841</td>\n      <td>0.000374</td>\n      <td>(0.8916093782077561, 0.892073504578041)</td>\n      <td>0.940094</td>\n      <td>0.000089</td>\n      <td>(0.9400392674556989, 0.9401496983417253)</td>\n      <td>0.857027</td>\n      <td>0.000636</td>\n      <td>...</td>\n      <td>0.418605</td>\n      <td>0.000000</td>\n      <td>(0.4186046511627907, 0.4186046511627907)</td>\n      <td>0.8</td>\n      <td>0.0</td>\n      <td>(0.8, 0.8)</td>\n      <td>648.0</td>\n      <td>79.0</td>\n      <td>12798.0</td>\n      <td>13367.0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>harassment</td>\n      <td>tf_idf</td>\n      <td>0.841425</td>\n      <td>0.000436</td>\n      <td>(0.8411545901034245, 0.8416953177716364)</td>\n      <td>0.887627</td>\n      <td>0.000099</td>\n      <td>(0.8875659028939921, 0.8876888950414937)</td>\n      <td>0.776939</td>\n      <td>0.001277</td>\n      <td>...</td>\n      <td>0.509302</td>\n      <td>0.007354</td>\n      <td>(0.5047441860465116, 0.513860465116279)</td>\n      <td>0.8</td>\n      <td>0.0</td>\n      <td>(0.8, 0.8)</td>\n      <td>648.0</td>\n      <td>79.0</td>\n      <td>12798.0</td>\n      <td>13367.0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>hate</td>\n      <td>glove</td>\n      <td>0.932036</td>\n      <td>0.000219</td>\n      <td>(0.9319009918665166, 0.9321719508672479)</td>\n      <td>0.747700</td>\n      <td>0.000700</td>\n      <td>(0.7472665422467533, 0.74813399381358)</td>\n      <td>0.181555</td>\n      <td>0.004138</td>\n      <td>...</td>\n      <td>0.003670</td>\n      <td>0.000000</td>\n      <td>(0.0036697247706422025, 0.0036697247706422033)</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>(0.0, 0.0)</td>\n      <td>584.0</td>\n      <td>67.0</td>\n      <td>11805.0</td>\n      <td>12322.0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>hate</td>\n      <td>ngram</td>\n      <td>0.939898</td>\n      <td>0.000175</td>\n      <td>(0.9397897014513805, 0.9400068750485584)</td>\n      <td>0.845559</td>\n      <td>0.000209</td>\n      <td>(0.8454292276271003, 0.8456877848736626)</td>\n      <td>0.342456</td>\n      <td>0.002477</td>\n      <td>...</td>\n      <td>0.007339</td>\n      <td>0.000000</td>\n      <td>(0.007339449541284405, 0.007339449541284407)</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>(0.0, 0.0)</td>\n      <td>584.0</td>\n      <td>67.0</td>\n      <td>11805.0</td>\n      <td>12322.0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>hate</td>\n      <td>tf_idf</td>\n      <td>0.936976</td>\n      <td>0.000074</td>\n      <td>(0.9369303948864447, 0.9370223276629939)</td>\n      <td>0.755414</td>\n      <td>0.000466</td>\n      <td>(0.7551246294842189, 0.7557026138438756)</td>\n      <td>0.246324</td>\n      <td>0.002781</td>\n      <td>...</td>\n      <td>0.005505</td>\n      <td>0.000000</td>\n      <td>(0.005504587155963303, 0.005504587155963303)</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>(0.0, 0.0)</td>\n      <td>584.0</td>\n      <td>67.0</td>\n      <td>11805.0</td>\n      <td>12322.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>15 rows × 69 columns</p>\n</div>"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_dev = result_val.groupby(['dataset', 'task_name']).agg(\n",
    "    acc_mean=('acc ', 'mean'),\n",
    "    acc_std=('acc ', 'std'),\n",
    "    acc_ci=('acc ', ci_sample),\n",
    "    auc_roc_mean=('auc_roc ', 'mean'),\n",
    "    auc_roc_std=('auc_roc ', 'std'),\n",
    "    auc_roc_ci=('auc_roc ', ci_sample),\n",
    "    f1_mean=('f1 ', 'mean'),\n",
    "    f1_std=('f1 ', 'std'),\n",
    "    f1_ci=('f1 ', ci_sample),\n",
    "    precision_mean=('precision ', 'mean'),\n",
    "    precision_std=('precision ', 'std'),\n",
    "    precision_ci=('precision ', ci_sample),\n",
    "    recall_mean=('recall ', 'mean'),\n",
    "    recall_std=('recall ', 'std'),\n",
    "    recall_ci=('recall ', ci_sample),\n",
    "    disparate_impact_favorable_06_mean=('disparate_impact_favorable_06 ', np.mean),\n",
    "    disparate_impact_favorable_06_std=('disparate_impact_favorable_06 ', np.std),\n",
    "    disparate_impact_favorable_06_ci=('disparate_impact_favorable_06 ',ci_sample),\n",
    "    disparate_impact_favorable_08_mean=('disparate_impact_favorable_08 ', np.mean),\n",
    "    disparate_impact_favorable_08_std=('disparate_impact_favorable_08 ',np.std),\n",
    "    disparate_impact_favorable_08_ci=('disparate_impact_favorable_08 ', ci_sample),\n",
    "    disparate_impact_unfavorable_06_mean=('disparate_impact_unfavorable_06 ', np.mean),\n",
    "    disparate_impact_unfavorable_06_std=('disparate_impact_unfavorable_06 ', np.std),\n",
    "    disparate_impact_unfavorable_06_ci=('disparate_impact_unfavorable_06 ',ci_sample),\n",
    "    disparate_impact_unfavorable_08_mean=('disparate_impact_unfavorable_08 ', np.mean),\n",
    "    disparate_impact_unfavorable_08_std=('disparate_impact_unfavorable_08 ',np.std),\n",
    "    disparate_impact_unfavorable_08_ci=('disparate_impact_unfavorable_08 ', ci_sample),\n",
    "    fnr_priv_06_mean=('fnr_priv_06 ', np.mean),\n",
    "    fnr_priv_06_std=('fnr_priv_06 ', np.std),\n",
    "    fnr_priv_06_ci=('fnr_priv_06 ', ci_sample),\n",
    "    fnr_priv_08_mean=('fnr_priv_08 ', np.mean),\n",
    "    fnr_priv_08_std=('fnr_priv_08 ', np.std),\n",
    "    fnr_priv_08_ci=('fnr_priv_08 ', ci_sample),\n",
    "    fnr_total_06_mean=('fnr_total_06 ', np.mean),\n",
    "    fnr_total_06_std=('fnr_total_06 ', np.std),\n",
    "    fnr_total_06_ci=('fnr_total_06 ', ci_sample),\n",
    "    fnr_total_08_mean=('fnr_total_08 ', np.mean),\n",
    "    fnr_total_08_std=('fnr_total_08 ', np.std),\n",
    "    fnr_total_08_ci=('fnr_total_08 ', ci_sample),\n",
    "    fnr_unpriv_06_mean=('fnr_unpriv_06 ', np.mean),\n",
    "    fnr_unpriv_06_std=('fnr_unpriv_06 ', np.std),\n",
    "    fnr_unpriv_06_ci=('fnr_unpriv_06 ', ci_sample),\n",
    "    fnr_unpriv_08_mean=('fnr_unpriv_08 ', np.mean),\n",
    "    fnr_unpriv_08_std=('fnr_unpriv_08 ', np.std),\n",
    "    fnr_unpriv_08_ci=('fnr_unpriv_08 ', ci_sample),\n",
    "    fpr_priv_06_mean=('fpr_priv_06 ', np.mean),\n",
    "    fpr_priv_06_std=('fpr_priv_06 ', np.std),\n",
    "    fpr_priv_06_ci=('fpr_priv_06 ', ci_sample),\n",
    "    fpr_priv_08_mean=('fpr_priv_08 ', np.mean),\n",
    "    fpr_priv_08_std=('fpr_priv_08 ', np.std),\n",
    "    fpr_priv_08_ci=('fpr_priv_08 ', ci_sample),\n",
    "    fpr_total_06_mean=('fpr_total_06 ', np.mean),\n",
    "    fpr_total_06_std=('fpr_total_06 ', np.std),\n",
    "    fpr_total_06_ci=('fpr_total_06 ', ci_sample),\n",
    "    fpr_total_08_mean=('fpr_total_08 ', np.mean),\n",
    "    fpr_total_08_std=('fpr_total_08 ', np.std),\n",
    "    fpr_total_08_ci=('fpr_total_08 ', ci_sample),\n",
    "    fpr_unpriv_06_mean=('fpr_unpriv_06 ', np.mean),\n",
    "    fpr_unpriv_06_std=('fpr_unpriv_06 ', np.std),\n",
    "    fpr_unpriv_06_ci=('fpr_unpriv_06 ', ci_sample),\n",
    "    fpr_unpriv_08_mean=('fpr_unpriv_08 ', np.mean),\n",
    "    fpr_unpriv_08_std=('fpr_unpriv_08 ', np.std),\n",
    "    fpr_unpriv_08_ci=('fpr_unpriv_08 ', ci_sample),\n",
    "    priv_n_06=('priv_n_06 ', np.median),\n",
    "    priv_n_08=('priv_n_08 ', np.median),\n",
    "    unpriv_n_06=('unpriv_n_06 ', np.median),\n",
    "    unpriv_n_08=('unpriv_n_08 ', np.median),\n",
    ").reset_index().sort_values(by=['dataset', 'task_name'])\n",
    "temp_dev.to_csv(f'{root}/log_reg_stats/dev_agg_seed_results.csv', index=False)\n",
    "temp_dev"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "       dataset task_name  acc_mean   acc_std  \\\n0     davidson     glove  0.887328  0.001309   \n1     davidson     ngram  0.939312  0.000982   \n2     davidson    tf_idf  0.871943  0.000333   \n3       founta     glove  0.899826  0.000561   \n4       founta     ngram  0.935095  0.000388   \n5       founta    tf_idf  0.895674  0.000542   \n6      golbeck     glove  0.759383  0.000939   \n7      golbeck     ngram  0.779177  0.000737   \n8      golbeck    tf_idf  0.776555  0.000434   \n9   harassment     glove  0.839305  0.000563   \n10  harassment     ngram  0.900022  0.000316   \n11  harassment    tf_idf  0.844452  0.000553   \n12        hate     glove  0.931770  0.000121   \n13        hate     ngram  0.938413  0.000126   \n14        hate    tf_idf  0.936387  0.000172   \n\n                                      acc_ci  auc_roc_mean  auc_roc_std  \\\n0   (0.8865165472814914, 0.8881393312623623)      0.907370     0.001982   \n1    (0.9387033864172455, 0.939920111068869)      0.977848     0.000251   \n2    (0.871736729002732, 0.8721499086849511)      0.885099     0.000678   \n3   (0.8994781086922924, 0.9001732518194875)      0.935428     0.000305   \n4   (0.9348543425436208, 0.9353352526788524)      0.959789     0.000147   \n5    (0.8953386787548121, 0.896010207735056)      0.914075     0.000249   \n6   (0.7588012211568581, 0.7599648244135154)      0.587727     0.004376   \n7   (0.7787204922927863, 0.7796342550025933)      0.683468     0.000562   \n8   (0.7762865351041158, 0.7768239928881328)      0.639059     0.000607   \n9   (0.8389562937668006, 0.8396544364044031)      0.902648     0.000171   \n10  (0.8998266913462144, 0.9002179406117935)      0.944787     0.000152   \n11  (0.8441088246050079, 0.8447949482259551)      0.891626     0.000082   \n12  (0.9316953591971178, 0.9318448849053602)      0.756342     0.000862   \n13  (0.9383348847680312, 0.9384913300223131)      0.841897     0.000265   \n14  (0.9362804639135192, 0.9364937794412782)      0.723379     0.000534   \n\n                                  auc_roc_ci   f1_mean    f1_std  ...  \\\n0   (0.9061414733989916, 0.9085987910167492)  0.932861  0.000617  ...   \n1   (0.9776921062772947, 0.9780029387170595)  0.963891  0.000545  ...   \n2       (0.8846788297635392, 0.885519479658)  0.926375  0.000190  ...   \n3    (0.9352383406918522, 0.935616776152325)  0.808424  0.002534  ...   \n4   (0.9596976988256661, 0.9598804242669853)  0.877733  0.000623  ...   \n5   (0.9139211009213997, 0.9142291595270562)  0.787913  0.001371  ...   \n6    (0.5850141425823744, 0.590439031531853)  0.048368  0.006537  ...   \n7   (0.6831197540509362, 0.6838166554224829)  0.272135  0.005568  ...   \n8   (0.6386833417438993, 0.6394356262660494)  0.261335  0.006933  ...   \n9   (0.9025419199844602, 0.9027544176200626)  0.786571  0.001216  ...   \n10     (0.94469346987638, 0.944881439333825)  0.866759  0.000603  ...   \n11  (0.8915750930635723, 0.8916773011357991)  0.780454  0.001552  ...   \n12  (0.7558075653299694, 0.7568761242642994)  0.148105  0.006397  ...   \n13  (0.8417321095648744, 0.8420611729439758)  0.306105  0.003005  ...   \n14  (0.7230483734468918, 0.7237105167050857)  0.234180  0.002997  ...   \n\n   fpr_unpriv_06_mean  fpr_unpriv_06_std  \\\n0            0.446154           0.079446   \n1            0.392308           0.024325   \n2            0.846154           0.000000   \n3            0.294118           0.000000   \n4            0.235294           0.000000   \n5            0.294118           0.000000   \n6            0.000000           0.000000   \n7            0.000000           0.000000   \n8            0.000000           0.000000   \n9            0.607692           0.016217   \n10           0.384615           0.000000   \n11           0.473077           0.018579   \n12           0.001518           0.000800   \n13           0.017647           0.000917   \n14           0.011385           0.000000   \n\n                                fpr_unpriv_06_ci fpr_unpriv_08_mean  \\\n0     (0.39691282158097413, 0.49539487072671834)           0.000000   \n1     (0.37723076923076926, 0.40738461538461546)           0.000000   \n2       (0.8461538461538461, 0.8461538461538461)           0.000000   \n3     (0.29411764705882343, 0.29411764705882354)           0.000000   \n4       (0.2352941176470588, 0.2352941176470588)           0.000000   \n5     (0.29411764705882343, 0.29411764705882354)           0.000000   \n6                                     (0.0, 0.0)                NaN   \n7                                     (0.0, 0.0)                NaN   \n8                                     (0.0, 0.0)                NaN   \n9       (0.5976410256410256, 0.6177435897435897)           1.000000   \n10    (0.38461538461538464, 0.38461538461538464)           0.500000   \n11     (0.4615617328690854, 0.48459211328476093)           1.000000   \n12   (0.00102213788741303, 0.002013915243516762)           0.000000   \n13    (0.01707894697266835, 0.01821517067439048)           0.021311   \n14  (0.011385199240986715, 0.011385199240986715)           0.016393   \n\n    fpr_unpriv_08_std                              fpr_unpriv_08_ci priv_n_06  \\\n0            0.000000                                    (0.0, 0.0)     502.0   \n1            0.000000                                    (0.0, 0.0)     502.0   \n2            0.000000                                    (0.0, 0.0)     502.0   \n3            0.000000                                    (0.0, 0.0)     117.0   \n4            0.000000                                    (0.0, 0.0)     117.0   \n5            0.000000                                    (0.0, 0.0)     117.0   \n6                 NaN                                    (nan, nan)      13.0   \n7                 NaN                                    (nan, nan)      13.0   \n8                 NaN                                    (nan, nan)      13.0   \n9            0.000000                                    (1.0, 1.0)     596.0   \n10           0.000000                                    (0.5, 0.5)     596.0   \n11           0.000000                                    (1.0, 1.0)     596.0   \n12           0.000000                                    (0.0, 0.0)     569.0   \n13           0.007919   (0.016403361550757674, 0.02621958926891444)     569.0   \n14           0.000000  (0.016393442622950817, 0.016393442622950817)     569.0   \n\n    priv_n_08  unpriv_n_06 unpriv_n_08  \n0        74.0       1968.0      2396.0  \n1        74.0       1968.0      2396.0  \n2        74.0       1968.0      2396.0  \n3         7.0       9061.0      9171.0  \n4         7.0       9061.0      9171.0  \n5         7.0       9061.0      9171.0  \n6         NaN       1932.0         NaN  \n7         NaN       1932.0         NaN  \n8         NaN       1932.0         NaN  \n9        70.0      12850.0     13376.0  \n10       70.0      12850.0     13376.0  \n11       70.0      12850.0     13376.0  \n12       66.0      11820.0     12323.0  \n13       66.0      11820.0     12323.0  \n14       66.0      11820.0     12323.0  \n\n[15 rows x 69 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dataset</th>\n      <th>task_name</th>\n      <th>acc_mean</th>\n      <th>acc_std</th>\n      <th>acc_ci</th>\n      <th>auc_roc_mean</th>\n      <th>auc_roc_std</th>\n      <th>auc_roc_ci</th>\n      <th>f1_mean</th>\n      <th>f1_std</th>\n      <th>...</th>\n      <th>fpr_unpriv_06_mean</th>\n      <th>fpr_unpriv_06_std</th>\n      <th>fpr_unpriv_06_ci</th>\n      <th>fpr_unpriv_08_mean</th>\n      <th>fpr_unpriv_08_std</th>\n      <th>fpr_unpriv_08_ci</th>\n      <th>priv_n_06</th>\n      <th>priv_n_08</th>\n      <th>unpriv_n_06</th>\n      <th>unpriv_n_08</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>davidson</td>\n      <td>glove</td>\n      <td>0.887328</td>\n      <td>0.001309</td>\n      <td>(0.8865165472814914, 0.8881393312623623)</td>\n      <td>0.907370</td>\n      <td>0.001982</td>\n      <td>(0.9061414733989916, 0.9085987910167492)</td>\n      <td>0.932861</td>\n      <td>0.000617</td>\n      <td>...</td>\n      <td>0.446154</td>\n      <td>0.079446</td>\n      <td>(0.39691282158097413, 0.49539487072671834)</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>(0.0, 0.0)</td>\n      <td>502.0</td>\n      <td>74.0</td>\n      <td>1968.0</td>\n      <td>2396.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>davidson</td>\n      <td>ngram</td>\n      <td>0.939312</td>\n      <td>0.000982</td>\n      <td>(0.9387033864172455, 0.939920111068869)</td>\n      <td>0.977848</td>\n      <td>0.000251</td>\n      <td>(0.9776921062772947, 0.9780029387170595)</td>\n      <td>0.963891</td>\n      <td>0.000545</td>\n      <td>...</td>\n      <td>0.392308</td>\n      <td>0.024325</td>\n      <td>(0.37723076923076926, 0.40738461538461546)</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>(0.0, 0.0)</td>\n      <td>502.0</td>\n      <td>74.0</td>\n      <td>1968.0</td>\n      <td>2396.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>davidson</td>\n      <td>tf_idf</td>\n      <td>0.871943</td>\n      <td>0.000333</td>\n      <td>(0.871736729002732, 0.8721499086849511)</td>\n      <td>0.885099</td>\n      <td>0.000678</td>\n      <td>(0.8846788297635392, 0.885519479658)</td>\n      <td>0.926375</td>\n      <td>0.000190</td>\n      <td>...</td>\n      <td>0.846154</td>\n      <td>0.000000</td>\n      <td>(0.8461538461538461, 0.8461538461538461)</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>(0.0, 0.0)</td>\n      <td>502.0</td>\n      <td>74.0</td>\n      <td>1968.0</td>\n      <td>2396.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>founta</td>\n      <td>glove</td>\n      <td>0.899826</td>\n      <td>0.000561</td>\n      <td>(0.8994781086922924, 0.9001732518194875)</td>\n      <td>0.935428</td>\n      <td>0.000305</td>\n      <td>(0.9352383406918522, 0.935616776152325)</td>\n      <td>0.808424</td>\n      <td>0.002534</td>\n      <td>...</td>\n      <td>0.294118</td>\n      <td>0.000000</td>\n      <td>(0.29411764705882343, 0.29411764705882354)</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>(0.0, 0.0)</td>\n      <td>117.0</td>\n      <td>7.0</td>\n      <td>9061.0</td>\n      <td>9171.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>founta</td>\n      <td>ngram</td>\n      <td>0.935095</td>\n      <td>0.000388</td>\n      <td>(0.9348543425436208, 0.9353352526788524)</td>\n      <td>0.959789</td>\n      <td>0.000147</td>\n      <td>(0.9596976988256661, 0.9598804242669853)</td>\n      <td>0.877733</td>\n      <td>0.000623</td>\n      <td>...</td>\n      <td>0.235294</td>\n      <td>0.000000</td>\n      <td>(0.2352941176470588, 0.2352941176470588)</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>(0.0, 0.0)</td>\n      <td>117.0</td>\n      <td>7.0</td>\n      <td>9061.0</td>\n      <td>9171.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>founta</td>\n      <td>tf_idf</td>\n      <td>0.895674</td>\n      <td>0.000542</td>\n      <td>(0.8953386787548121, 0.896010207735056)</td>\n      <td>0.914075</td>\n      <td>0.000249</td>\n      <td>(0.9139211009213997, 0.9142291595270562)</td>\n      <td>0.787913</td>\n      <td>0.001371</td>\n      <td>...</td>\n      <td>0.294118</td>\n      <td>0.000000</td>\n      <td>(0.29411764705882343, 0.29411764705882354)</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>(0.0, 0.0)</td>\n      <td>117.0</td>\n      <td>7.0</td>\n      <td>9061.0</td>\n      <td>9171.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>golbeck</td>\n      <td>glove</td>\n      <td>0.759383</td>\n      <td>0.000939</td>\n      <td>(0.7588012211568581, 0.7599648244135154)</td>\n      <td>0.587727</td>\n      <td>0.004376</td>\n      <td>(0.5850141425823744, 0.590439031531853)</td>\n      <td>0.048368</td>\n      <td>0.006537</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>(0.0, 0.0)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>13.0</td>\n      <td>NaN</td>\n      <td>1932.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>golbeck</td>\n      <td>ngram</td>\n      <td>0.779177</td>\n      <td>0.000737</td>\n      <td>(0.7787204922927863, 0.7796342550025933)</td>\n      <td>0.683468</td>\n      <td>0.000562</td>\n      <td>(0.6831197540509362, 0.6838166554224829)</td>\n      <td>0.272135</td>\n      <td>0.005568</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>(0.0, 0.0)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>13.0</td>\n      <td>NaN</td>\n      <td>1932.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>golbeck</td>\n      <td>tf_idf</td>\n      <td>0.776555</td>\n      <td>0.000434</td>\n      <td>(0.7762865351041158, 0.7768239928881328)</td>\n      <td>0.639059</td>\n      <td>0.000607</td>\n      <td>(0.6386833417438993, 0.6394356262660494)</td>\n      <td>0.261335</td>\n      <td>0.006933</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>(0.0, 0.0)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>13.0</td>\n      <td>NaN</td>\n      <td>1932.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>harassment</td>\n      <td>glove</td>\n      <td>0.839305</td>\n      <td>0.000563</td>\n      <td>(0.8389562937668006, 0.8396544364044031)</td>\n      <td>0.902648</td>\n      <td>0.000171</td>\n      <td>(0.9025419199844602, 0.9027544176200626)</td>\n      <td>0.786571</td>\n      <td>0.001216</td>\n      <td>...</td>\n      <td>0.607692</td>\n      <td>0.016217</td>\n      <td>(0.5976410256410256, 0.6177435897435897)</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>(1.0, 1.0)</td>\n      <td>596.0</td>\n      <td>70.0</td>\n      <td>12850.0</td>\n      <td>13376.0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>harassment</td>\n      <td>ngram</td>\n      <td>0.900022</td>\n      <td>0.000316</td>\n      <td>(0.8998266913462144, 0.9002179406117935)</td>\n      <td>0.944787</td>\n      <td>0.000152</td>\n      <td>(0.94469346987638, 0.944881439333825)</td>\n      <td>0.866759</td>\n      <td>0.000603</td>\n      <td>...</td>\n      <td>0.384615</td>\n      <td>0.000000</td>\n      <td>(0.38461538461538464, 0.38461538461538464)</td>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>(0.5, 0.5)</td>\n      <td>596.0</td>\n      <td>70.0</td>\n      <td>12850.0</td>\n      <td>13376.0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>harassment</td>\n      <td>tf_idf</td>\n      <td>0.844452</td>\n      <td>0.000553</td>\n      <td>(0.8441088246050079, 0.8447949482259551)</td>\n      <td>0.891626</td>\n      <td>0.000082</td>\n      <td>(0.8915750930635723, 0.8916773011357991)</td>\n      <td>0.780454</td>\n      <td>0.001552</td>\n      <td>...</td>\n      <td>0.473077</td>\n      <td>0.018579</td>\n      <td>(0.4615617328690854, 0.48459211328476093)</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>(1.0, 1.0)</td>\n      <td>596.0</td>\n      <td>70.0</td>\n      <td>12850.0</td>\n      <td>13376.0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>hate</td>\n      <td>glove</td>\n      <td>0.931770</td>\n      <td>0.000121</td>\n      <td>(0.9316953591971178, 0.9318448849053602)</td>\n      <td>0.756342</td>\n      <td>0.000862</td>\n      <td>(0.7558075653299694, 0.7568761242642994)</td>\n      <td>0.148105</td>\n      <td>0.006397</td>\n      <td>...</td>\n      <td>0.001518</td>\n      <td>0.000800</td>\n      <td>(0.00102213788741303, 0.002013915243516762)</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>(0.0, 0.0)</td>\n      <td>569.0</td>\n      <td>66.0</td>\n      <td>11820.0</td>\n      <td>12323.0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>hate</td>\n      <td>ngram</td>\n      <td>0.938413</td>\n      <td>0.000126</td>\n      <td>(0.9383348847680312, 0.9384913300223131)</td>\n      <td>0.841897</td>\n      <td>0.000265</td>\n      <td>(0.8417321095648744, 0.8420611729439758)</td>\n      <td>0.306105</td>\n      <td>0.003005</td>\n      <td>...</td>\n      <td>0.017647</td>\n      <td>0.000917</td>\n      <td>(0.01707894697266835, 0.01821517067439048)</td>\n      <td>0.021311</td>\n      <td>0.007919</td>\n      <td>(0.016403361550757674, 0.02621958926891444)</td>\n      <td>569.0</td>\n      <td>66.0</td>\n      <td>11820.0</td>\n      <td>12323.0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>hate</td>\n      <td>tf_idf</td>\n      <td>0.936387</td>\n      <td>0.000172</td>\n      <td>(0.9362804639135192, 0.9364937794412782)</td>\n      <td>0.723379</td>\n      <td>0.000534</td>\n      <td>(0.7230483734468918, 0.7237105167050857)</td>\n      <td>0.234180</td>\n      <td>0.002997</td>\n      <td>...</td>\n      <td>0.011385</td>\n      <td>0.000000</td>\n      <td>(0.011385199240986715, 0.011385199240986715)</td>\n      <td>0.016393</td>\n      <td>0.000000</td>\n      <td>(0.016393442622950817, 0.016393442622950817)</td>\n      <td>569.0</td>\n      <td>66.0</td>\n      <td>11820.0</td>\n      <td>12323.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>15 rows × 69 columns</p>\n</div>"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_test = result_test.groupby(['dataset', 'task_name']).agg(\n",
    "    acc_mean=('acc ', 'mean'),\n",
    "    acc_std=('acc ', 'std'),\n",
    "    acc_ci=('acc ', ci_sample),\n",
    "    auc_roc_mean=('auc_roc ', 'mean'),\n",
    "    auc_roc_std=('auc_roc ', 'std'),\n",
    "    auc_roc_ci=('auc_roc ', ci_sample),\n",
    "    f1_mean=('f1 ', 'mean'),\n",
    "    f1_std=('f1 ', 'std'),\n",
    "    f1_ci=('f1 ', ci_sample),\n",
    "    precision_mean=('precision ', 'mean'),\n",
    "    precision_std=('precision ', 'std'),\n",
    "    precision_ci=('precision ', ci_sample),\n",
    "    recall_mean=('recall ', 'mean'),\n",
    "    recall_std=('recall ', 'std'),\n",
    "    recall_ci=('recall ', ci_sample),\n",
    "    disparate_impact_favorable_06_mean=('disparate_impact_favorable_06 ', np.mean),\n",
    "    disparate_impact_favorable_06_std=('disparate_impact_favorable_06 ', np.std),\n",
    "    disparate_impact_favorable_06_ci=('disparate_impact_favorable_06 ',ci_sample),\n",
    "    disparate_impact_favorable_08_mean=('disparate_impact_favorable_08 ', np.mean),\n",
    "    disparate_impact_favorable_08_std=('disparate_impact_favorable_08 ',np.std),\n",
    "    disparate_impact_favorable_08_ci=('disparate_impact_favorable_08 ', ci_sample),\n",
    "    disparate_impact_unfavorable_06_mean=('disparate_impact_unfavorable_06 ', np.mean),\n",
    "    disparate_impact_unfavorable_06_std=('disparate_impact_unfavorable_06 ', np.std),\n",
    "    disparate_impact_unfavorable_06_ci=('disparate_impact_unfavorable_06 ',ci_sample),\n",
    "    disparate_impact_unfavorable_08_mean=('disparate_impact_unfavorable_08 ', np.mean),\n",
    "    disparate_impact_unfavorable_08_std=('disparate_impact_unfavorable_08 ',np.std),\n",
    "    disparate_impact_unfavorable_08_ci=('disparate_impact_unfavorable_08 ', ci_sample),\n",
    "    fnr_priv_06_mean=('fnr_priv_06 ', np.mean),\n",
    "    fnr_priv_06_std=('fnr_priv_06 ', np.std),\n",
    "    fnr_priv_06_ci=('fnr_priv_06 ', ci_sample),\n",
    "    fnr_priv_08_mean=('fnr_priv_08 ', np.mean),\n",
    "    fnr_priv_08_std=('fnr_priv_08 ', np.std),\n",
    "    fnr_priv_08_ci=('fnr_priv_08 ', ci_sample),\n",
    "    fnr_total_06_mean=('fnr_total_06 ', np.mean),\n",
    "    fnr_total_06_std=('fnr_total_06 ', np.std),\n",
    "    fnr_total_06_ci=('fnr_total_06 ', ci_sample),\n",
    "    fnr_total_08_mean=('fnr_total_08 ', np.mean),\n",
    "    fnr_total_08_std=('fnr_total_08 ', np.std),\n",
    "    fnr_total_08_ci=('fnr_total_08 ', ci_sample),\n",
    "    fnr_unpriv_06_mean=('fnr_unpriv_06 ', np.mean),\n",
    "    fnr_unpriv_06_std=('fnr_unpriv_06 ', np.std),\n",
    "    fnr_unpriv_06_ci=('fnr_unpriv_06 ', ci_sample),\n",
    "    fnr_unpriv_08_mean=('fnr_unpriv_08 ', np.mean),\n",
    "    fnr_unpriv_08_std=('fnr_unpriv_08 ', np.std),\n",
    "    fnr_unpriv_08_ci=('fnr_unpriv_08 ', ci_sample),\n",
    "    fpr_priv_06_mean=('fpr_priv_06 ', np.mean),\n",
    "    fpr_priv_06_std=('fpr_priv_06 ', np.std),\n",
    "    fpr_priv_06_ci=('fpr_priv_06 ', ci_sample),\n",
    "    fpr_priv_08_mean=('fpr_priv_08 ', np.mean),\n",
    "    fpr_priv_08_std=('fpr_priv_08 ', np.std),\n",
    "    fpr_priv_08_ci=('fpr_priv_08 ', ci_sample),\n",
    "    fpr_total_06_mean=('fpr_total_06 ', np.mean),\n",
    "    fpr_total_06_std=('fpr_total_06 ', np.std),\n",
    "    fpr_total_06_ci=('fpr_total_06 ', ci_sample),\n",
    "    fpr_total_08_mean=('fpr_total_08 ', np.mean),\n",
    "    fpr_total_08_std=('fpr_total_08 ', np.std),\n",
    "    fpr_total_08_ci=('fpr_total_08 ', ci_sample),\n",
    "    fpr_unpriv_06_mean=('fpr_unpriv_06 ', np.mean),\n",
    "    fpr_unpriv_06_std=('fpr_unpriv_06 ', np.std),\n",
    "    fpr_unpriv_06_ci=('fpr_unpriv_06 ', ci_sample),\n",
    "    fpr_unpriv_08_mean=('fpr_unpriv_08 ', np.mean),\n",
    "    fpr_unpriv_08_std=('fpr_unpriv_08 ', np.std),\n",
    "    fpr_unpriv_08_ci=('fpr_unpriv_08 ', ci_sample),\n",
    "    priv_n_06=('priv_n_06 ', np.median),\n",
    "    priv_n_08=('priv_n_08 ', np.median),\n",
    "    unpriv_n_06=('unpriv_n_06 ', np.median),\n",
    "    unpriv_n_08=('unpriv_n_08 ', np.median),\n",
    ").reset_index().sort_values(by=['dataset', 'task_name'])\n",
    "temp_test.to_csv(f'{root}/log_reg_stats/test_agg_seed_results.csv', index=False)\n",
    "temp_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "temp.to_csv('../data/contextualize_results/stats_on_tasks.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "models_dict = {'twitter': {'twitter_es_reg_nb5_h5_is_bal_pos_seed_', 'twitter_es_reg_nb0_h1_bal_seed_', 'twitter_es_vanilla_bal_seed_'},\n",
    "                'twitter_harass': {'twitter_harass_es_reg_nb5_h5_is_bal_pos_seed_','twitter_harass_es_reg_nb0_h1_bal_seed_','twitter_harass_es_vanilla_bal_seed_'},\n",
    "               'gab': {'majority_gab_es_vanilla_bal_seed_', 'majority_gab_es_reg_nb0_h1_bal_seed_', 'majority_gab_es_reg_nb5_h5_is_bal_pos_seed_'},\n",
    "               'ws': {'ws_es_vanilla_bal_seed_', 'ws_es_reg_nb0_h1_bal_seed_','ws_es_reg_nb5_h5_is_bal_pos_seed_'},\n",
    "               'nyt': {},\n",
    "               }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "                                            model            task  acc_mean  \\\n14                twitter_es_reg_nb0_h1_bal_seed_         twitter  0.740416   \n18         twitter_es_reg_nb5_h5_is_bal_pos_seed_         twitter  0.780775   \n22                   twitter_es_vanilla_bal_seed_         twitter  0.797047   \n27         twitter_harass_es_reg_nb0_h1_bal_seed_  twitter_harass  0.699126   \n32  twitter_harass_es_reg_nb5_h5_is_bal_pos_seed_  twitter_harass  0.798674   \n37            twitter_harass_es_vanilla_bal_seed_  twitter_harass  0.802177   \n0            majority_gab_es_reg_nb0_h1_bal_seed_             gab  0.862108   \n4     majority_gab_es_reg_nb5_h5_is_bal_pos_seed_             gab  0.881928   \n8               majority_gab_es_vanilla_bal_seed_             gab  0.882831   \n42                     ws_es_reg_nb0_h1_bal_seed_              ws  0.895282   \n46              ws_es_reg_nb5_h5_is_bal_pos_seed_              ws  0.898782   \n50                        ws_es_vanilla_bal_seed_              ws  0.908676   \n\n     acc_std                                    acc_ci  auc_roc_mean  \\\n14  0.013804  (0.7318604701540944, 0.7489718244091427)      0.851023   \n18  0.020300  (0.7681924281851983, 0.7933570971767733)      0.861636   \n22  0.016549   (0.786789594562518, 0.8073037988596702)      0.867681   \n27  0.108810  (0.6316849387126151, 0.7665673015302938)      0.913960   \n32  0.010532  (0.7921467442694817, 0.8052019930481517)      0.951445   \n37  0.013758  (0.7936501224867812, 0.8107044653825811)      0.951767   \n0   0.012026  (0.8546544856593332, 0.8695623818105465)      0.874507   \n4   0.020701  (0.8690969473006679, 0.8947584743860789)      0.886380   \n8   0.015758  (0.8730642575235151, 0.8925983930788944)      0.891738   \n42  0.012831  (0.8873287167761869, 0.9032344491294447)      0.880330   \n46  0.010888  (0.8920342011868992, 0.9055304867887476)      0.882531   \n50  0.006214  (0.9048244305638931, 0.9125271676096227)      0.876880   \n\n    auc_roc_std                                auc_roc_ci   f1_mean    f1_std  \\\n14     0.005828  (0.8474105131000839, 0.8546345486463854)  0.400923  0.009749   \n18     0.006482  (0.8576180444353265, 0.8656529654302911)  0.434504  0.012329   \n22     0.012040  (0.8602187635615669, 0.8751441492098829)  0.452028  0.014638   \n27     0.096742  (0.8539989489968628, 0.9739213422284224)  0.724786  0.059479   \n32     0.004280  (0.9487928326630026, 0.9540978517512333)  0.790119  0.007984   \n37     0.005010  (0.9486616697289599, 0.9548716952451467)  0.793076  0.010919   \n0      0.007422  (0.8699068385251156, 0.8791072046781914)  0.449921  0.018215   \n4      0.010175  (0.8800729619505216, 0.8926863453965002)  0.480698  0.030686   \n8      0.007496  (0.8870917266629993, 0.8963837414831942)  0.488232  0.021547   \n42     0.009994  (0.8741355728690714, 0.8865244461242809)  0.547665  0.020910   \n46     0.007200   (0.8780681767780336, 0.886993551617028)  0.553661  0.019914   \n50     0.011693   (0.869632653017099, 0.8841280307435848)  0.563915  0.026242   \n\n    ... disparate_impact_06_mean  disparate_impact_06_std  \\\n14  ...                 0.857148                 0.044112   \n18  ...                 0.826937                 0.060878   \n22  ...                 0.840683                 0.035621   \n27  ...                      inf                      NaN   \n32  ...                41.367705                 8.509532   \n37  ...                35.196131                11.226862   \n0   ...                 1.581563                 0.638837   \n4   ...                 1.286723                 0.038792   \n8   ...                 1.582046                 0.701131   \n42  ...                      NaN                      NaN   \n46  ...                      NaN                      NaN   \n50  ...                      NaN                      NaN   \n\n                      disparate_impact_06_ci disparate_impact_08_mean  \\\n14   (0.8298066061381789, 0.884489005007136)                 0.962671   \n18  (0.7892042012454198, 0.8646696652235802)                 0.866940   \n22  (0.8186045992182596, 0.8627610571687874)                 0.874632   \n27                                (nan, nan)                      inf   \n32    (36.09344237057658, 46.64196711913195)                      inf   \n37    (28.23764979208232, 42.15461160470309)                      inf   \n0   (1.1856077090048187, 1.9775184225582472)                      inf   \n4   (1.2626797696530172, 1.3107662170699763)                      inf   \n8   (1.1474801485394046, 2.0166115835064615)                      inf   \n42                                (nan, nan)                      NaN   \n46                                (nan, nan)                      NaN   \n50                                (nan, nan)                      NaN   \n\n    disparate_impact_08_std                    disparate_impact_08_ci  \\\n14                 0.097553    (0.902206724586391, 1.023134586881783)   \n18                 0.114318  (0.7960850410549574, 0.9377945217594896)   \n22                 0.077094  (0.8268481619460178, 0.9224149791085426)   \n27                      NaN                                (nan, nan)   \n32                      NaN                                (nan, nan)   \n37                      NaN                                (nan, nan)   \n0                       NaN                                (nan, nan)   \n4                       NaN                                (nan, nan)   \n8                       NaN                                (nan, nan)   \n42                      NaN                                (nan, nan)   \n46                      NaN                                (nan, nan)   \n50                      NaN                                (nan, nan)   \n\n   priv_n_06  priv_n_08  unpriv_n_06 unpriv_n_08  \n14     525.0       64.0       9904.0     10365.0  \n18     525.0       64.0       9904.0     10365.0  \n22     525.0       64.0       9904.0     10365.0  \n27     656.0       77.0      12847.0     13426.0  \n32     656.0       77.0      12847.0     13426.0  \n37     656.0       77.0      12847.0     13426.0  \n0        3.0        1.0       1657.0      1659.0  \n4        3.0        1.0       1657.0      1659.0  \n8        3.0        1.0       1657.0      1659.0  \n42       NaN        NaN          NaN         NaN  \n46       NaN        NaN          NaN         NaN  \n50       NaN        NaN          NaN         NaN  \n\n[12 rows x 36 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>task</th>\n      <th>acc_mean</th>\n      <th>acc_std</th>\n      <th>acc_ci</th>\n      <th>auc_roc_mean</th>\n      <th>auc_roc_std</th>\n      <th>auc_roc_ci</th>\n      <th>f1_mean</th>\n      <th>f1_std</th>\n      <th>...</th>\n      <th>disparate_impact_06_mean</th>\n      <th>disparate_impact_06_std</th>\n      <th>disparate_impact_06_ci</th>\n      <th>disparate_impact_08_mean</th>\n      <th>disparate_impact_08_std</th>\n      <th>disparate_impact_08_ci</th>\n      <th>priv_n_06</th>\n      <th>priv_n_08</th>\n      <th>unpriv_n_06</th>\n      <th>unpriv_n_08</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>14</th>\n      <td>twitter_es_reg_nb0_h1_bal_seed_</td>\n      <td>twitter</td>\n      <td>0.740416</td>\n      <td>0.013804</td>\n      <td>(0.7318604701540944, 0.7489718244091427)</td>\n      <td>0.851023</td>\n      <td>0.005828</td>\n      <td>(0.8474105131000839, 0.8546345486463854)</td>\n      <td>0.400923</td>\n      <td>0.009749</td>\n      <td>...</td>\n      <td>0.857148</td>\n      <td>0.044112</td>\n      <td>(0.8298066061381789, 0.884489005007136)</td>\n      <td>0.962671</td>\n      <td>0.097553</td>\n      <td>(0.902206724586391, 1.023134586881783)</td>\n      <td>525.0</td>\n      <td>64.0</td>\n      <td>9904.0</td>\n      <td>10365.0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>twitter_es_reg_nb5_h5_is_bal_pos_seed_</td>\n      <td>twitter</td>\n      <td>0.780775</td>\n      <td>0.020300</td>\n      <td>(0.7681924281851983, 0.7933570971767733)</td>\n      <td>0.861636</td>\n      <td>0.006482</td>\n      <td>(0.8576180444353265, 0.8656529654302911)</td>\n      <td>0.434504</td>\n      <td>0.012329</td>\n      <td>...</td>\n      <td>0.826937</td>\n      <td>0.060878</td>\n      <td>(0.7892042012454198, 0.8646696652235802)</td>\n      <td>0.866940</td>\n      <td>0.114318</td>\n      <td>(0.7960850410549574, 0.9377945217594896)</td>\n      <td>525.0</td>\n      <td>64.0</td>\n      <td>9904.0</td>\n      <td>10365.0</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>twitter_es_vanilla_bal_seed_</td>\n      <td>twitter</td>\n      <td>0.797047</td>\n      <td>0.016549</td>\n      <td>(0.786789594562518, 0.8073037988596702)</td>\n      <td>0.867681</td>\n      <td>0.012040</td>\n      <td>(0.8602187635615669, 0.8751441492098829)</td>\n      <td>0.452028</td>\n      <td>0.014638</td>\n      <td>...</td>\n      <td>0.840683</td>\n      <td>0.035621</td>\n      <td>(0.8186045992182596, 0.8627610571687874)</td>\n      <td>0.874632</td>\n      <td>0.077094</td>\n      <td>(0.8268481619460178, 0.9224149791085426)</td>\n      <td>525.0</td>\n      <td>64.0</td>\n      <td>9904.0</td>\n      <td>10365.0</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>twitter_harass_es_reg_nb0_h1_bal_seed_</td>\n      <td>twitter_harass</td>\n      <td>0.699126</td>\n      <td>0.108810</td>\n      <td>(0.6316849387126151, 0.7665673015302938)</td>\n      <td>0.913960</td>\n      <td>0.096742</td>\n      <td>(0.8539989489968628, 0.9739213422284224)</td>\n      <td>0.724786</td>\n      <td>0.059479</td>\n      <td>...</td>\n      <td>inf</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>inf</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>656.0</td>\n      <td>77.0</td>\n      <td>12847.0</td>\n      <td>13426.0</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>twitter_harass_es_reg_nb5_h5_is_bal_pos_seed_</td>\n      <td>twitter_harass</td>\n      <td>0.798674</td>\n      <td>0.010532</td>\n      <td>(0.7921467442694817, 0.8052019930481517)</td>\n      <td>0.951445</td>\n      <td>0.004280</td>\n      <td>(0.9487928326630026, 0.9540978517512333)</td>\n      <td>0.790119</td>\n      <td>0.007984</td>\n      <td>...</td>\n      <td>41.367705</td>\n      <td>8.509532</td>\n      <td>(36.09344237057658, 46.64196711913195)</td>\n      <td>inf</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>656.0</td>\n      <td>77.0</td>\n      <td>12847.0</td>\n      <td>13426.0</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>twitter_harass_es_vanilla_bal_seed_</td>\n      <td>twitter_harass</td>\n      <td>0.802177</td>\n      <td>0.013758</td>\n      <td>(0.7936501224867812, 0.8107044653825811)</td>\n      <td>0.951767</td>\n      <td>0.005010</td>\n      <td>(0.9486616697289599, 0.9548716952451467)</td>\n      <td>0.793076</td>\n      <td>0.010919</td>\n      <td>...</td>\n      <td>35.196131</td>\n      <td>11.226862</td>\n      <td>(28.23764979208232, 42.15461160470309)</td>\n      <td>inf</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>656.0</td>\n      <td>77.0</td>\n      <td>12847.0</td>\n      <td>13426.0</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>majority_gab_es_reg_nb0_h1_bal_seed_</td>\n      <td>gab</td>\n      <td>0.862108</td>\n      <td>0.012026</td>\n      <td>(0.8546544856593332, 0.8695623818105465)</td>\n      <td>0.874507</td>\n      <td>0.007422</td>\n      <td>(0.8699068385251156, 0.8791072046781914)</td>\n      <td>0.449921</td>\n      <td>0.018215</td>\n      <td>...</td>\n      <td>1.581563</td>\n      <td>0.638837</td>\n      <td>(1.1856077090048187, 1.9775184225582472)</td>\n      <td>inf</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1657.0</td>\n      <td>1659.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>majority_gab_es_reg_nb5_h5_is_bal_pos_seed_</td>\n      <td>gab</td>\n      <td>0.881928</td>\n      <td>0.020701</td>\n      <td>(0.8690969473006679, 0.8947584743860789)</td>\n      <td>0.886380</td>\n      <td>0.010175</td>\n      <td>(0.8800729619505216, 0.8926863453965002)</td>\n      <td>0.480698</td>\n      <td>0.030686</td>\n      <td>...</td>\n      <td>1.286723</td>\n      <td>0.038792</td>\n      <td>(1.2626797696530172, 1.3107662170699763)</td>\n      <td>inf</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1657.0</td>\n      <td>1659.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>majority_gab_es_vanilla_bal_seed_</td>\n      <td>gab</td>\n      <td>0.882831</td>\n      <td>0.015758</td>\n      <td>(0.8730642575235151, 0.8925983930788944)</td>\n      <td>0.891738</td>\n      <td>0.007496</td>\n      <td>(0.8870917266629993, 0.8963837414831942)</td>\n      <td>0.488232</td>\n      <td>0.021547</td>\n      <td>...</td>\n      <td>1.582046</td>\n      <td>0.701131</td>\n      <td>(1.1474801485394046, 2.0166115835064615)</td>\n      <td>inf</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1657.0</td>\n      <td>1659.0</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>ws_es_reg_nb0_h1_bal_seed_</td>\n      <td>ws</td>\n      <td>0.895282</td>\n      <td>0.012831</td>\n      <td>(0.8873287167761869, 0.9032344491294447)</td>\n      <td>0.880330</td>\n      <td>0.009994</td>\n      <td>(0.8741355728690714, 0.8865244461242809)</td>\n      <td>0.547665</td>\n      <td>0.020910</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>ws_es_reg_nb5_h5_is_bal_pos_seed_</td>\n      <td>ws</td>\n      <td>0.898782</td>\n      <td>0.010888</td>\n      <td>(0.8920342011868992, 0.9055304867887476)</td>\n      <td>0.882531</td>\n      <td>0.007200</td>\n      <td>(0.8780681767780336, 0.886993551617028)</td>\n      <td>0.553661</td>\n      <td>0.019914</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>ws_es_vanilla_bal_seed_</td>\n      <td>ws</td>\n      <td>0.908676</td>\n      <td>0.006214</td>\n      <td>(0.9048244305638931, 0.9125271676096227)</td>\n      <td>0.876880</td>\n      <td>0.011693</td>\n      <td>(0.869632653017099, 0.8841280307435848)</td>\n      <td>0.563915</td>\n      <td>0.026242</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>12 rows × 36 columns</p>\n</div>"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_to_learner_results = None\n",
    "for task in models_dict.keys():\n",
    "    if task_to_learner_results is None:\n",
    "        task_to_learner_results = temp[(temp['model'].isin(models_dict[task])) & (temp['task'] == task)]\n",
    "    else:\n",
    "        task_to_learner_results = pd.concat([task_to_learner_results, temp[(temp['model'].isin(models_dict[task])) & (temp['task'] == task)]])\n",
    "task_to_learner_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "other_tasks_results = temp[(temp['model'].isin(models_dict['twitter'])) & (temp['task'].isin({'gab', 'ws', 'twitter_harass', 'nyt'}))]\n",
    "other_tasks_results = pd.concat([other_tasks_results, temp[(temp['model'].isin(models_dict['gab'])) & (temp['task'].isin({'twitter', 'twitter_harass','ws', 'nyt'}))]])\n",
    "other_tasks_results = pd.concat([other_tasks_results, temp[(temp['model'].isin(models_dict['twitter_harass'])) & (temp['task'].isin({'twitter', 'gab', 'ws', 'nyt'}))]])\n",
    "other_tasks_results = pd.concat([other_tasks_results, temp[(temp['model'].isin(models_dict['ws'])) & (temp['task'].isin({'twitter', 'twitter_harass','gab', 'nyt'}))]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "                                            model     task  acc_mean  \\\n12                twitter_es_reg_nb0_h1_bal_seed_      gab  0.628976   \n13                twitter_es_reg_nb0_h1_bal_seed_      nyt  0.692862   \n15                twitter_es_reg_nb0_h1_bal_seed_       ws  0.742009   \n16         twitter_es_reg_nb5_h5_is_bal_pos_seed_      gab  0.697771   \n17         twitter_es_reg_nb5_h5_is_bal_pos_seed_      nyt  0.794993   \n19         twitter_es_reg_nb5_h5_is_bal_pos_seed_       ws  0.771081   \n20                   twitter_es_vanilla_bal_seed_      gab  0.766506   \n21                   twitter_es_vanilla_bal_seed_      nyt  0.639087   \n23                   twitter_es_vanilla_bal_seed_       ws  0.770015   \n1            majority_gab_es_reg_nb0_h1_bal_seed_      nyt  0.917362   \n2            majority_gab_es_reg_nb0_h1_bal_seed_  twitter  0.799079   \n3            majority_gab_es_reg_nb0_h1_bal_seed_       ws  0.872146   \n5     majority_gab_es_reg_nb5_h5_is_bal_pos_seed_      nyt  0.872833   \n6     majority_gab_es_reg_nb5_h5_is_bal_pos_seed_  twitter  0.827117   \n7     majority_gab_es_reg_nb5_h5_is_bal_pos_seed_       ws  0.866667   \n9               majority_gab_es_vanilla_bal_seed_      nyt  0.783710   \n10              majority_gab_es_vanilla_bal_seed_  twitter  0.826139   \n11              majority_gab_es_vanilla_bal_seed_       ws  0.855251   \n24         twitter_harass_es_reg_nb0_h1_bal_seed_      gab  0.379699   \n25         twitter_harass_es_reg_nb0_h1_bal_seed_      nyt  0.514942   \n26         twitter_harass_es_reg_nb0_h1_bal_seed_  twitter  0.406118   \n28         twitter_harass_es_reg_nb0_h1_bal_seed_       ws  0.446119   \n29  twitter_harass_es_reg_nb5_h5_is_bal_pos_seed_      gab  0.560843   \n30  twitter_harass_es_reg_nb5_h5_is_bal_pos_seed_      nyt  0.788420   \n31  twitter_harass_es_reg_nb5_h5_is_bal_pos_seed_  twitter  0.509243   \n33  twitter_harass_es_reg_nb5_h5_is_bal_pos_seed_       ws  0.638661   \n34            twitter_harass_es_vanilla_bal_seed_      gab  0.602169   \n35            twitter_harass_es_vanilla_bal_seed_      nyt  0.670174   \n36            twitter_harass_es_vanilla_bal_seed_  twitter  0.517413   \n38            twitter_harass_es_vanilla_bal_seed_       ws  0.645662   \n39                     ws_es_reg_nb0_h1_bal_seed_      gab  0.864157   \n40                     ws_es_reg_nb0_h1_bal_seed_      nyt  0.950290   \n41                     ws_es_reg_nb0_h1_bal_seed_  twitter  0.837789   \n43              ws_es_reg_nb5_h5_is_bal_pos_seed_      gab  0.876506   \n44              ws_es_reg_nb5_h5_is_bal_pos_seed_      nyt  0.946913   \n45              ws_es_reg_nb5_h5_is_bal_pos_seed_  twitter  0.839908   \n47                        ws_es_vanilla_bal_seed_      gab  0.886506   \n48                        ws_es_vanilla_bal_seed_      nyt  0.925312   \n49                        ws_es_vanilla_bal_seed_  twitter  0.852622   \n\n     acc_std                                    acc_ci  auc_roc_mean  \\\n12  0.036343   (0.6064502124205815, 0.651501594808334)      0.818889   \n13  0.067703  (0.6508996380141923, 0.7348249996669673)      0.000000   \n15  0.025167  (0.7264106936761919, 0.7576075711639908)      0.748519   \n16  0.047310  (0.6684477917724448, 0.7270943769022539)      0.821897   \n17  0.068235  (0.7527000530041628, 0.8372854542422141)      0.000000   \n19  0.030103  (0.7524226540514041, 0.7897386853702091)      0.750957   \n20  0.026450  (0.7501121545689884, 0.7828998936237828)      0.856527   \n21  0.088046  (0.5845155413088085, 0.6936583717346699)      0.000000   \n23  0.025544  (0.7541827073545508, 0.7858477340457535)      0.781363   \n1   0.024158  (0.9023890204927413, 0.9323356171884182)      0.000000   \n2   0.017829  (0.7880290003327614, 0.8101299794351934)      0.739081   \n3   0.009383  (0.8663306797820927, 0.8779615576608297)      0.828538   \n5   0.039768   (0.848184690859489, 0.8974819758071777)      0.000000   \n6   0.020782  (0.8142356392423945, 0.8399977484266054)      0.742117   \n7   0.012307   (0.8590387528229122, 0.874294580510421)      0.831201   \n9   0.082726  (0.7324361957562131, 0.8349840940988593)      0.000000   \n10  0.024281  (0.8110894393905235, 0.8411878642819284)      0.765980   \n11  0.019379  (0.8432400574146646, 0.8672622256903582)      0.817381   \n24  0.095714  (0.3203747184857717, 0.4390228718756741)      0.742436   \n25  0.152156  (0.4206346927711272, 0.6092493651998875)      0.000000   \n26  0.105845  (0.3405144276676427, 0.4717206859578248)      0.632587   \n28  0.117335  (0.3733935018879902, 0.5188439410343842)      0.675195   \n29  0.039372  (0.5364404975067922, 0.5852462494811594)      0.777038   \n30  0.074252  (0.7423986963211718, 0.8344418833889732)      0.000000   \n31  0.014008   (0.5005610762980869, 0.517925835198701)      0.665595   \n33  0.030009  (0.6200609537697843, 0.6572602030034271)      0.700591   \n34  0.036257  (0.5796961682181324, 0.6246411811794578)      0.804706   \n35  0.136414  (0.5856235273886329, 0.7547242986983235)      0.000000   \n36  0.016805  (0.5069968279531226, 0.5278291381030668)      0.671612   \n38  0.045367  (0.6175434173891281, 0.6737807835241136)      0.752339   \n39  0.015398  (0.8546125512364103, 0.8737007017756379)      0.831959   \n40  0.018182    (0.9390205514168576, 0.96155915872807)      0.000000   \n41  0.016418  (0.8276130445855631, 0.8479646713987116)      0.703172   \n43  0.017684   (0.8655451389050631, 0.887466909287708)      0.820443   \n44  0.021891    (0.933344683125569, 0.960481403830953)      0.000000   \n45  0.015874  (0.8300692747146365, 0.8497466232621589)      0.668893   \n47  0.009150   (0.880834729090929, 0.8921773191018424)      0.824919   \n48  0.032900  (0.9049199483400787, 0.9457032400657184)      0.000000   \n49  0.011199  (0.8456814051444388, 0.8595635847874818)      0.682889   \n\n    auc_roc_std                                auc_roc_ci   f1_mean    f1_std  \\\n12     0.007008  (0.8145454343478087, 0.8232331002209861)  0.276995  0.014425   \n13     0.000000                                (0.0, 0.0)  0.000000  0.000000   \n15     0.014860   (0.739308099673421, 0.7577289373636157)  0.333568  0.027191   \n16     0.006244  (0.8180272449425315, 0.8257673052161579)  0.310001  0.022413   \n17     0.000000                                (0.0, 0.0)  0.000000  0.000000   \n19     0.017452   (0.740140144293877, 0.7617734359530367)  0.336322  0.026036   \n20     0.003998  (0.8540483260302403, 0.8590049000279665)  0.365528  0.015665   \n21     0.000000                                (0.0, 0.0)  0.000000  0.000000   \n23     0.007915  (0.7764569630741037, 0.7862685829847756)  0.367121  0.020248   \n1      0.000000                                (0.0, 0.0)  0.000000  0.000000   \n2      0.009118  (0.7334297289437248, 0.7447327526196015)  0.310914  0.016020   \n3      0.010412   (0.822084203132161, 0.8349908206095297)  0.462081  0.043819   \n5      0.000000                                (0.0, 0.0)  0.000000  0.000000   \n6      0.008340  (0.7369475537143811, 0.7472862318210093)  0.338319  0.016135   \n7      0.010391  (0.8247609941413466, 0.8376416649279793)  0.456342  0.021947   \n9      0.000000                                (0.0, 0.0)  0.000000  0.000000   \n10     0.012306  (0.7583530118646027, 0.7736074387796548)  0.364111  0.023082   \n11     0.014362  (0.8084793695094268, 0.8262832135864895)  0.444361  0.033555   \n24     0.068906   (0.6997275726981227, 0.785144647868619)  0.206410  0.020160   \n25     0.000000                                (0.0, 0.0)  0.000000  0.000000   \n26     0.054127  (0.5990382481337116, 0.6661351030052333)  0.258826  0.024589   \n28     0.030385  (0.6563619978598194, 0.6940273658628775)  0.249156  0.021532   \n29     0.012195  (0.7694790245808799, 0.7845960585478834)  0.247210  0.010643   \n30     0.000000                                (0.0, 0.0)  0.000000  0.000000   \n31     0.009027   (0.6600004388941796, 0.671190202289641)  0.283021  0.006467   \n33     0.027587  (0.6834927972414768, 0.7176895389408593)  0.277852  0.022509   \n34     0.014344  (0.7958156887433998, 0.8135964265347919)  0.274922  0.013678   \n35     0.000000                                (0.0, 0.0)  0.000000  0.000000   \n36     0.005114  (0.6684420221495045, 0.6747810040427719)  0.290563  0.006767   \n38     0.027014  (0.7355953062433136, 0.7690818067671328)  0.310706  0.014215   \n39     0.009972  (0.8257782144034154, 0.8381394088943963)  0.389587  0.016350   \n40     0.000000                                (0.0, 0.0)  0.000000  0.000000   \n41     0.011715  (0.6959110560752069, 0.7104336283354482)  0.240423  0.018180   \n43     0.007849  (0.8155782147433719, 0.8253083659329152)  0.404088  0.018860   \n44     0.000000                                (0.0, 0.0)  0.000000  0.000000   \n45     0.013680  (0.6604144330821795, 0.6773719489549745)  0.236558  0.030968   \n47     0.014433  (0.8159736343584921, 0.8338643650644545)  0.411479  0.013811   \n48     0.000000                                (0.0, 0.0)  0.000000  0.000000   \n49     0.021370  (0.6696437944150544, 0.6961348786190702)  0.254986  0.025921   \n\n    ... disparate_impact_06_mean  disparate_impact_06_std  \\\n12  ...                 0.856639                 0.060978   \n13  ...                      NaN                      NaN   \n15  ...                      NaN                      NaN   \n16  ...                 1.219403                 0.461532   \n17  ...                      NaN                      NaN   \n19  ...                      NaN                      NaN   \n20  ...                 1.142336                 0.538762   \n21  ...                      NaN                      NaN   \n23  ...                      NaN                      NaN   \n1   ...                      NaN                      NaN   \n2   ...                 1.757463                 0.192340   \n3   ...                      NaN                      NaN   \n5   ...                      NaN                      NaN   \n6   ...                 1.484461                 0.201475   \n7   ...                      NaN                      NaN   \n9   ...                      NaN                      NaN   \n10  ...                 1.395322                 0.214769   \n11  ...                      NaN                      NaN   \n24  ...                      inf                      NaN   \n25  ...                      NaN                      NaN   \n26  ...                      inf                      NaN   \n28  ...                      NaN                      NaN   \n29  ...                 1.503440                 0.136999   \n30  ...                      NaN                      NaN   \n31  ...                53.686251                13.176558   \n33  ...                      NaN                      NaN   \n34  ...                 1.608087                 0.125840   \n35  ...                      NaN                      NaN   \n36  ...                47.484888                12.397625   \n38  ...                      NaN                      NaN   \n39  ...                 1.422269                 0.652811   \n40  ...                      NaN                      NaN   \n41  ...                 1.312394                 0.111740   \n43  ...                 1.184882                 0.219707   \n44  ...                      NaN                      NaN   \n45  ...                 1.314945                 0.110908   \n47  ...                 1.736451                 0.822002   \n48  ...                      NaN                      NaN   \n49  ...                 1.148876                 0.065224   \n\n                      disparate_impact_06_ci disparate_impact_08_mean  \\\n12  (0.8188442563563496, 0.8944327502821539)                      inf   \n13                                (nan, nan)                      NaN   \n15                                (nan, nan)                      NaN   \n16  (0.9333420557721496, 1.5054630136303855)                      inf   \n17                                (nan, nan)                      NaN   \n19                                (nan, nan)                      NaN   \n20  (0.8084076131365296, 1.4762634791990161)                      inf   \n21                                (nan, nan)                      NaN   \n23                                (nan, nan)                      NaN   \n1                                 (nan, nan)                      NaN   \n2   (1.6382492846163017, 1.8766758280603493)                 2.507590   \n3                                 (nan, nan)                      NaN   \n5                                 (nan, nan)                      NaN   \n6    (1.359585089673893, 1.6093359280251855)                 2.070144   \n7                                 (nan, nan)                      NaN   \n9                                 (nan, nan)                      NaN   \n10   (1.2622069849227628, 1.528437971842096)                 1.886746   \n11                                (nan, nan)                      NaN   \n24                                (nan, nan)                      inf   \n25                                (nan, nan)                      NaN   \n26                                (nan, nan)                      inf   \n28                                (nan, nan)                      NaN   \n29   (1.418527037234916, 1.5883528662050364)                      inf   \n30                                (nan, nan)                      NaN   \n31    (45.51933526788563, 61.85316574180741)                      inf   \n33                                (nan, nan)                      NaN   \n34  (1.5300901852478757, 1.6860836228390288)                      inf   \n35                                (nan, nan)                      NaN   \n36    (39.80076077659822, 55.16901574467264)                      inf   \n38                                (nan, nan)                      NaN   \n39  (1.0176524195040277, 1.8268859027651339)                      inf   \n40                                (nan, nan)                      NaN   \n41  (1.2431365385628228, 1.3816505491772186)                 1.604506   \n43     (1.04870648399513, 1.321058150887188)                      inf   \n44                                (nan, nan)                      NaN   \n45   (1.2462035176620634, 1.383685913577819)                 1.617468   \n47   (1.2269691864052354, 2.245933650046182)                      inf   \n48                                (nan, nan)                      NaN   \n49  (1.1084497942979241, 1.1893017272230977)                 1.425536   \n\n    disparate_impact_08_std                    disparate_impact_08_ci  \\\n12                      NaN                                (nan, nan)   \n13                      NaN                                (nan, nan)   \n15                      NaN                                (nan, nan)   \n16                      NaN                                (nan, nan)   \n17                      NaN                                (nan, nan)   \n19                      NaN                                (nan, nan)   \n20                      NaN                                (nan, nan)   \n21                      NaN                                (nan, nan)   \n23                      NaN                                (nan, nan)   \n1                       NaN                                (nan, nan)   \n2                  0.451256  (2.2278989080454092, 2.7872813465869815)   \n3                       NaN                                (nan, nan)   \n5                       NaN                                (nan, nan)   \n6                  0.510267   (1.753876940510465, 2.3864110749132044)   \n7                       NaN                                (nan, nan)   \n9                       NaN                                (nan, nan)   \n10                 0.465513  (1.5982180553720318, 2.1752740423674437)   \n11                      NaN                                (nan, nan)   \n24                      NaN                                (nan, nan)   \n25                      NaN                                (nan, nan)   \n26                      NaN                                (nan, nan)   \n28                      NaN                                (nan, nan)   \n29                      NaN                                (nan, nan)   \n30                      NaN                                (nan, nan)   \n31                      NaN                                (nan, nan)   \n33                      NaN                                (nan, nan)   \n34                      NaN                                (nan, nan)   \n35                      NaN                                (nan, nan)   \n36                      NaN                                (nan, nan)   \n38                      NaN                                (nan, nan)   \n39                      NaN                                (nan, nan)   \n40                      NaN                                (nan, nan)   \n41                 0.102379   (1.5410506382865614, 1.667961196666625)   \n43                      NaN                                (nan, nan)   \n44                      NaN                                (nan, nan)   \n45                 0.223165  (1.4791492612021604, 1.7557876735994413)   \n47                      NaN                                (nan, nan)   \n48                      NaN                                (nan, nan)   \n49                 0.130200  (1.3448370255274684, 1.5062350348675102)   \n\n   priv_n_06  priv_n_08  unpriv_n_06 unpriv_n_08  \n12       3.0        1.0       1657.0      1659.0  \n13       NaN        NaN          NaN         NaN  \n15       NaN        NaN          NaN         NaN  \n16       3.0        1.0       1657.0      1659.0  \n17       NaN        NaN          NaN         NaN  \n19       NaN        NaN          NaN         NaN  \n20       3.0        1.0       1657.0      1659.0  \n21       NaN        NaN          NaN         NaN  \n23       NaN        NaN          NaN         NaN  \n1        NaN        NaN          NaN         NaN  \n2      525.0       64.0       9904.0     10365.0  \n3        NaN        NaN          NaN         NaN  \n5        NaN        NaN          NaN         NaN  \n6      525.0       64.0       9904.0     10365.0  \n7        NaN        NaN          NaN         NaN  \n9        NaN        NaN          NaN         NaN  \n10     525.0       64.0       9904.0     10365.0  \n11       NaN        NaN          NaN         NaN  \n24       3.0        1.0       1657.0      1659.0  \n25       NaN        NaN          NaN         NaN  \n26     525.0       64.0       9904.0     10365.0  \n28       NaN        NaN          NaN         NaN  \n29       3.0        1.0       1657.0      1659.0  \n30       NaN        NaN          NaN         NaN  \n31     525.0       64.0       9904.0     10365.0  \n33       NaN        NaN          NaN         NaN  \n34       3.0        1.0       1657.0      1659.0  \n35       NaN        NaN          NaN         NaN  \n36     525.0       64.0       9904.0     10365.0  \n38       NaN        NaN          NaN         NaN  \n39       3.0        1.0       1657.0      1659.0  \n40       NaN        NaN          NaN         NaN  \n41     525.0       64.0       9904.0     10365.0  \n43       3.0        1.0       1657.0      1659.0  \n44       NaN        NaN          NaN         NaN  \n45     525.0       64.0       9904.0     10365.0  \n47       3.0        1.0       1657.0      1659.0  \n48       NaN        NaN          NaN         NaN  \n49     525.0       64.0       9904.0     10365.0  \n\n[39 rows x 36 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>task</th>\n      <th>acc_mean</th>\n      <th>acc_std</th>\n      <th>acc_ci</th>\n      <th>auc_roc_mean</th>\n      <th>auc_roc_std</th>\n      <th>auc_roc_ci</th>\n      <th>f1_mean</th>\n      <th>f1_std</th>\n      <th>...</th>\n      <th>disparate_impact_06_mean</th>\n      <th>disparate_impact_06_std</th>\n      <th>disparate_impact_06_ci</th>\n      <th>disparate_impact_08_mean</th>\n      <th>disparate_impact_08_std</th>\n      <th>disparate_impact_08_ci</th>\n      <th>priv_n_06</th>\n      <th>priv_n_08</th>\n      <th>unpriv_n_06</th>\n      <th>unpriv_n_08</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>12</th>\n      <td>twitter_es_reg_nb0_h1_bal_seed_</td>\n      <td>gab</td>\n      <td>0.628976</td>\n      <td>0.036343</td>\n      <td>(0.6064502124205815, 0.651501594808334)</td>\n      <td>0.818889</td>\n      <td>0.007008</td>\n      <td>(0.8145454343478087, 0.8232331002209861)</td>\n      <td>0.276995</td>\n      <td>0.014425</td>\n      <td>...</td>\n      <td>0.856639</td>\n      <td>0.060978</td>\n      <td>(0.8188442563563496, 0.8944327502821539)</td>\n      <td>inf</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1657.0</td>\n      <td>1659.0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>twitter_es_reg_nb0_h1_bal_seed_</td>\n      <td>nyt</td>\n      <td>0.692862</td>\n      <td>0.067703</td>\n      <td>(0.6508996380141923, 0.7348249996669673)</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>(0.0, 0.0)</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>twitter_es_reg_nb0_h1_bal_seed_</td>\n      <td>ws</td>\n      <td>0.742009</td>\n      <td>0.025167</td>\n      <td>(0.7264106936761919, 0.7576075711639908)</td>\n      <td>0.748519</td>\n      <td>0.014860</td>\n      <td>(0.739308099673421, 0.7577289373636157)</td>\n      <td>0.333568</td>\n      <td>0.027191</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>twitter_es_reg_nb5_h5_is_bal_pos_seed_</td>\n      <td>gab</td>\n      <td>0.697771</td>\n      <td>0.047310</td>\n      <td>(0.6684477917724448, 0.7270943769022539)</td>\n      <td>0.821897</td>\n      <td>0.006244</td>\n      <td>(0.8180272449425315, 0.8257673052161579)</td>\n      <td>0.310001</td>\n      <td>0.022413</td>\n      <td>...</td>\n      <td>1.219403</td>\n      <td>0.461532</td>\n      <td>(0.9333420557721496, 1.5054630136303855)</td>\n      <td>inf</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1657.0</td>\n      <td>1659.0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>twitter_es_reg_nb5_h5_is_bal_pos_seed_</td>\n      <td>nyt</td>\n      <td>0.794993</td>\n      <td>0.068235</td>\n      <td>(0.7527000530041628, 0.8372854542422141)</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>(0.0, 0.0)</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>twitter_es_reg_nb5_h5_is_bal_pos_seed_</td>\n      <td>ws</td>\n      <td>0.771081</td>\n      <td>0.030103</td>\n      <td>(0.7524226540514041, 0.7897386853702091)</td>\n      <td>0.750957</td>\n      <td>0.017452</td>\n      <td>(0.740140144293877, 0.7617734359530367)</td>\n      <td>0.336322</td>\n      <td>0.026036</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>twitter_es_vanilla_bal_seed_</td>\n      <td>gab</td>\n      <td>0.766506</td>\n      <td>0.026450</td>\n      <td>(0.7501121545689884, 0.7828998936237828)</td>\n      <td>0.856527</td>\n      <td>0.003998</td>\n      <td>(0.8540483260302403, 0.8590049000279665)</td>\n      <td>0.365528</td>\n      <td>0.015665</td>\n      <td>...</td>\n      <td>1.142336</td>\n      <td>0.538762</td>\n      <td>(0.8084076131365296, 1.4762634791990161)</td>\n      <td>inf</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1657.0</td>\n      <td>1659.0</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>twitter_es_vanilla_bal_seed_</td>\n      <td>nyt</td>\n      <td>0.639087</td>\n      <td>0.088046</td>\n      <td>(0.5845155413088085, 0.6936583717346699)</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>(0.0, 0.0)</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>twitter_es_vanilla_bal_seed_</td>\n      <td>ws</td>\n      <td>0.770015</td>\n      <td>0.025544</td>\n      <td>(0.7541827073545508, 0.7858477340457535)</td>\n      <td>0.781363</td>\n      <td>0.007915</td>\n      <td>(0.7764569630741037, 0.7862685829847756)</td>\n      <td>0.367121</td>\n      <td>0.020248</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>majority_gab_es_reg_nb0_h1_bal_seed_</td>\n      <td>nyt</td>\n      <td>0.917362</td>\n      <td>0.024158</td>\n      <td>(0.9023890204927413, 0.9323356171884182)</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>(0.0, 0.0)</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>majority_gab_es_reg_nb0_h1_bal_seed_</td>\n      <td>twitter</td>\n      <td>0.799079</td>\n      <td>0.017829</td>\n      <td>(0.7880290003327614, 0.8101299794351934)</td>\n      <td>0.739081</td>\n      <td>0.009118</td>\n      <td>(0.7334297289437248, 0.7447327526196015)</td>\n      <td>0.310914</td>\n      <td>0.016020</td>\n      <td>...</td>\n      <td>1.757463</td>\n      <td>0.192340</td>\n      <td>(1.6382492846163017, 1.8766758280603493)</td>\n      <td>2.507590</td>\n      <td>0.451256</td>\n      <td>(2.2278989080454092, 2.7872813465869815)</td>\n      <td>525.0</td>\n      <td>64.0</td>\n      <td>9904.0</td>\n      <td>10365.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>majority_gab_es_reg_nb0_h1_bal_seed_</td>\n      <td>ws</td>\n      <td>0.872146</td>\n      <td>0.009383</td>\n      <td>(0.8663306797820927, 0.8779615576608297)</td>\n      <td>0.828538</td>\n      <td>0.010412</td>\n      <td>(0.822084203132161, 0.8349908206095297)</td>\n      <td>0.462081</td>\n      <td>0.043819</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>majority_gab_es_reg_nb5_h5_is_bal_pos_seed_</td>\n      <td>nyt</td>\n      <td>0.872833</td>\n      <td>0.039768</td>\n      <td>(0.848184690859489, 0.8974819758071777)</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>(0.0, 0.0)</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>majority_gab_es_reg_nb5_h5_is_bal_pos_seed_</td>\n      <td>twitter</td>\n      <td>0.827117</td>\n      <td>0.020782</td>\n      <td>(0.8142356392423945, 0.8399977484266054)</td>\n      <td>0.742117</td>\n      <td>0.008340</td>\n      <td>(0.7369475537143811, 0.7472862318210093)</td>\n      <td>0.338319</td>\n      <td>0.016135</td>\n      <td>...</td>\n      <td>1.484461</td>\n      <td>0.201475</td>\n      <td>(1.359585089673893, 1.6093359280251855)</td>\n      <td>2.070144</td>\n      <td>0.510267</td>\n      <td>(1.753876940510465, 2.3864110749132044)</td>\n      <td>525.0</td>\n      <td>64.0</td>\n      <td>9904.0</td>\n      <td>10365.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>majority_gab_es_reg_nb5_h5_is_bal_pos_seed_</td>\n      <td>ws</td>\n      <td>0.866667</td>\n      <td>0.012307</td>\n      <td>(0.8590387528229122, 0.874294580510421)</td>\n      <td>0.831201</td>\n      <td>0.010391</td>\n      <td>(0.8247609941413466, 0.8376416649279793)</td>\n      <td>0.456342</td>\n      <td>0.021947</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>majority_gab_es_vanilla_bal_seed_</td>\n      <td>nyt</td>\n      <td>0.783710</td>\n      <td>0.082726</td>\n      <td>(0.7324361957562131, 0.8349840940988593)</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>(0.0, 0.0)</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>majority_gab_es_vanilla_bal_seed_</td>\n      <td>twitter</td>\n      <td>0.826139</td>\n      <td>0.024281</td>\n      <td>(0.8110894393905235, 0.8411878642819284)</td>\n      <td>0.765980</td>\n      <td>0.012306</td>\n      <td>(0.7583530118646027, 0.7736074387796548)</td>\n      <td>0.364111</td>\n      <td>0.023082</td>\n      <td>...</td>\n      <td>1.395322</td>\n      <td>0.214769</td>\n      <td>(1.2622069849227628, 1.528437971842096)</td>\n      <td>1.886746</td>\n      <td>0.465513</td>\n      <td>(1.5982180553720318, 2.1752740423674437)</td>\n      <td>525.0</td>\n      <td>64.0</td>\n      <td>9904.0</td>\n      <td>10365.0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>majority_gab_es_vanilla_bal_seed_</td>\n      <td>ws</td>\n      <td>0.855251</td>\n      <td>0.019379</td>\n      <td>(0.8432400574146646, 0.8672622256903582)</td>\n      <td>0.817381</td>\n      <td>0.014362</td>\n      <td>(0.8084793695094268, 0.8262832135864895)</td>\n      <td>0.444361</td>\n      <td>0.033555</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>twitter_harass_es_reg_nb0_h1_bal_seed_</td>\n      <td>gab</td>\n      <td>0.379699</td>\n      <td>0.095714</td>\n      <td>(0.3203747184857717, 0.4390228718756741)</td>\n      <td>0.742436</td>\n      <td>0.068906</td>\n      <td>(0.6997275726981227, 0.785144647868619)</td>\n      <td>0.206410</td>\n      <td>0.020160</td>\n      <td>...</td>\n      <td>inf</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>inf</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1657.0</td>\n      <td>1659.0</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>twitter_harass_es_reg_nb0_h1_bal_seed_</td>\n      <td>nyt</td>\n      <td>0.514942</td>\n      <td>0.152156</td>\n      <td>(0.4206346927711272, 0.6092493651998875)</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>(0.0, 0.0)</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>twitter_harass_es_reg_nb0_h1_bal_seed_</td>\n      <td>twitter</td>\n      <td>0.406118</td>\n      <td>0.105845</td>\n      <td>(0.3405144276676427, 0.4717206859578248)</td>\n      <td>0.632587</td>\n      <td>0.054127</td>\n      <td>(0.5990382481337116, 0.6661351030052333)</td>\n      <td>0.258826</td>\n      <td>0.024589</td>\n      <td>...</td>\n      <td>inf</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>inf</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>525.0</td>\n      <td>64.0</td>\n      <td>9904.0</td>\n      <td>10365.0</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>twitter_harass_es_reg_nb0_h1_bal_seed_</td>\n      <td>ws</td>\n      <td>0.446119</td>\n      <td>0.117335</td>\n      <td>(0.3733935018879902, 0.5188439410343842)</td>\n      <td>0.675195</td>\n      <td>0.030385</td>\n      <td>(0.6563619978598194, 0.6940273658628775)</td>\n      <td>0.249156</td>\n      <td>0.021532</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>twitter_harass_es_reg_nb5_h5_is_bal_pos_seed_</td>\n      <td>gab</td>\n      <td>0.560843</td>\n      <td>0.039372</td>\n      <td>(0.5364404975067922, 0.5852462494811594)</td>\n      <td>0.777038</td>\n      <td>0.012195</td>\n      <td>(0.7694790245808799, 0.7845960585478834)</td>\n      <td>0.247210</td>\n      <td>0.010643</td>\n      <td>...</td>\n      <td>1.503440</td>\n      <td>0.136999</td>\n      <td>(1.418527037234916, 1.5883528662050364)</td>\n      <td>inf</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1657.0</td>\n      <td>1659.0</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>twitter_harass_es_reg_nb5_h5_is_bal_pos_seed_</td>\n      <td>nyt</td>\n      <td>0.788420</td>\n      <td>0.074252</td>\n      <td>(0.7423986963211718, 0.8344418833889732)</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>(0.0, 0.0)</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>twitter_harass_es_reg_nb5_h5_is_bal_pos_seed_</td>\n      <td>twitter</td>\n      <td>0.509243</td>\n      <td>0.014008</td>\n      <td>(0.5005610762980869, 0.517925835198701)</td>\n      <td>0.665595</td>\n      <td>0.009027</td>\n      <td>(0.6600004388941796, 0.671190202289641)</td>\n      <td>0.283021</td>\n      <td>0.006467</td>\n      <td>...</td>\n      <td>53.686251</td>\n      <td>13.176558</td>\n      <td>(45.51933526788563, 61.85316574180741)</td>\n      <td>inf</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>525.0</td>\n      <td>64.0</td>\n      <td>9904.0</td>\n      <td>10365.0</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>twitter_harass_es_reg_nb5_h5_is_bal_pos_seed_</td>\n      <td>ws</td>\n      <td>0.638661</td>\n      <td>0.030009</td>\n      <td>(0.6200609537697843, 0.6572602030034271)</td>\n      <td>0.700591</td>\n      <td>0.027587</td>\n      <td>(0.6834927972414768, 0.7176895389408593)</td>\n      <td>0.277852</td>\n      <td>0.022509</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>twitter_harass_es_vanilla_bal_seed_</td>\n      <td>gab</td>\n      <td>0.602169</td>\n      <td>0.036257</td>\n      <td>(0.5796961682181324, 0.6246411811794578)</td>\n      <td>0.804706</td>\n      <td>0.014344</td>\n      <td>(0.7958156887433998, 0.8135964265347919)</td>\n      <td>0.274922</td>\n      <td>0.013678</td>\n      <td>...</td>\n      <td>1.608087</td>\n      <td>0.125840</td>\n      <td>(1.5300901852478757, 1.6860836228390288)</td>\n      <td>inf</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1657.0</td>\n      <td>1659.0</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>twitter_harass_es_vanilla_bal_seed_</td>\n      <td>nyt</td>\n      <td>0.670174</td>\n      <td>0.136414</td>\n      <td>(0.5856235273886329, 0.7547242986983235)</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>(0.0, 0.0)</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>twitter_harass_es_vanilla_bal_seed_</td>\n      <td>twitter</td>\n      <td>0.517413</td>\n      <td>0.016805</td>\n      <td>(0.5069968279531226, 0.5278291381030668)</td>\n      <td>0.671612</td>\n      <td>0.005114</td>\n      <td>(0.6684420221495045, 0.6747810040427719)</td>\n      <td>0.290563</td>\n      <td>0.006767</td>\n      <td>...</td>\n      <td>47.484888</td>\n      <td>12.397625</td>\n      <td>(39.80076077659822, 55.16901574467264)</td>\n      <td>inf</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>525.0</td>\n      <td>64.0</td>\n      <td>9904.0</td>\n      <td>10365.0</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>twitter_harass_es_vanilla_bal_seed_</td>\n      <td>ws</td>\n      <td>0.645662</td>\n      <td>0.045367</td>\n      <td>(0.6175434173891281, 0.6737807835241136)</td>\n      <td>0.752339</td>\n      <td>0.027014</td>\n      <td>(0.7355953062433136, 0.7690818067671328)</td>\n      <td>0.310706</td>\n      <td>0.014215</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>ws_es_reg_nb0_h1_bal_seed_</td>\n      <td>gab</td>\n      <td>0.864157</td>\n      <td>0.015398</td>\n      <td>(0.8546125512364103, 0.8737007017756379)</td>\n      <td>0.831959</td>\n      <td>0.009972</td>\n      <td>(0.8257782144034154, 0.8381394088943963)</td>\n      <td>0.389587</td>\n      <td>0.016350</td>\n      <td>...</td>\n      <td>1.422269</td>\n      <td>0.652811</td>\n      <td>(1.0176524195040277, 1.8268859027651339)</td>\n      <td>inf</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1657.0</td>\n      <td>1659.0</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>ws_es_reg_nb0_h1_bal_seed_</td>\n      <td>nyt</td>\n      <td>0.950290</td>\n      <td>0.018182</td>\n      <td>(0.9390205514168576, 0.96155915872807)</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>(0.0, 0.0)</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>ws_es_reg_nb0_h1_bal_seed_</td>\n      <td>twitter</td>\n      <td>0.837789</td>\n      <td>0.016418</td>\n      <td>(0.8276130445855631, 0.8479646713987116)</td>\n      <td>0.703172</td>\n      <td>0.011715</td>\n      <td>(0.6959110560752069, 0.7104336283354482)</td>\n      <td>0.240423</td>\n      <td>0.018180</td>\n      <td>...</td>\n      <td>1.312394</td>\n      <td>0.111740</td>\n      <td>(1.2431365385628228, 1.3816505491772186)</td>\n      <td>1.604506</td>\n      <td>0.102379</td>\n      <td>(1.5410506382865614, 1.667961196666625)</td>\n      <td>525.0</td>\n      <td>64.0</td>\n      <td>9904.0</td>\n      <td>10365.0</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>ws_es_reg_nb5_h5_is_bal_pos_seed_</td>\n      <td>gab</td>\n      <td>0.876506</td>\n      <td>0.017684</td>\n      <td>(0.8655451389050631, 0.887466909287708)</td>\n      <td>0.820443</td>\n      <td>0.007849</td>\n      <td>(0.8155782147433719, 0.8253083659329152)</td>\n      <td>0.404088</td>\n      <td>0.018860</td>\n      <td>...</td>\n      <td>1.184882</td>\n      <td>0.219707</td>\n      <td>(1.04870648399513, 1.321058150887188)</td>\n      <td>inf</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1657.0</td>\n      <td>1659.0</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>ws_es_reg_nb5_h5_is_bal_pos_seed_</td>\n      <td>nyt</td>\n      <td>0.946913</td>\n      <td>0.021891</td>\n      <td>(0.933344683125569, 0.960481403830953)</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>(0.0, 0.0)</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>ws_es_reg_nb5_h5_is_bal_pos_seed_</td>\n      <td>twitter</td>\n      <td>0.839908</td>\n      <td>0.015874</td>\n      <td>(0.8300692747146365, 0.8497466232621589)</td>\n      <td>0.668893</td>\n      <td>0.013680</td>\n      <td>(0.6604144330821795, 0.6773719489549745)</td>\n      <td>0.236558</td>\n      <td>0.030968</td>\n      <td>...</td>\n      <td>1.314945</td>\n      <td>0.110908</td>\n      <td>(1.2462035176620634, 1.383685913577819)</td>\n      <td>1.617468</td>\n      <td>0.223165</td>\n      <td>(1.4791492612021604, 1.7557876735994413)</td>\n      <td>525.0</td>\n      <td>64.0</td>\n      <td>9904.0</td>\n      <td>10365.0</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>ws_es_vanilla_bal_seed_</td>\n      <td>gab</td>\n      <td>0.886506</td>\n      <td>0.009150</td>\n      <td>(0.880834729090929, 0.8921773191018424)</td>\n      <td>0.824919</td>\n      <td>0.014433</td>\n      <td>(0.8159736343584921, 0.8338643650644545)</td>\n      <td>0.411479</td>\n      <td>0.013811</td>\n      <td>...</td>\n      <td>1.736451</td>\n      <td>0.822002</td>\n      <td>(1.2269691864052354, 2.245933650046182)</td>\n      <td>inf</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1657.0</td>\n      <td>1659.0</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>ws_es_vanilla_bal_seed_</td>\n      <td>nyt</td>\n      <td>0.925312</td>\n      <td>0.032900</td>\n      <td>(0.9049199483400787, 0.9457032400657184)</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>(0.0, 0.0)</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>(nan, nan)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>ws_es_vanilla_bal_seed_</td>\n      <td>twitter</td>\n      <td>0.852622</td>\n      <td>0.011199</td>\n      <td>(0.8456814051444388, 0.8595635847874818)</td>\n      <td>0.682889</td>\n      <td>0.021370</td>\n      <td>(0.6696437944150544, 0.6961348786190702)</td>\n      <td>0.254986</td>\n      <td>0.025921</td>\n      <td>...</td>\n      <td>1.148876</td>\n      <td>0.065224</td>\n      <td>(1.1084497942979241, 1.1893017272230977)</td>\n      <td>1.425536</td>\n      <td>0.130200</td>\n      <td>(1.3448370255274684, 1.5062350348675102)</td>\n      <td>525.0</td>\n      <td>64.0</td>\n      <td>9904.0</td>\n      <td>10365.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>39 rows × 36 columns</p>\n</div>"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_tasks_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "task_to_learner_results.to_csv('../data/contextualize_results/task_to_learner.csv', index=False)\n",
    "other_tasks_results.to_csv('../data/contextualize_results/other_tasks_with_learner.csv', index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}