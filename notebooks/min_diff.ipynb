{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Imports\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import requests\n",
    "import tempfile\n",
    "import urllib\n",
    "import zipfile\n",
    "import string\n",
    "import re\n",
    "\n",
    "import tensorflow_model_remediation.min_diff as md\n",
    "from google.protobuf import text_format\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_model_analysis as tfma\n",
    "import tensorflow_data_validation as tfdv\n",
    "from tensorflow_model_analysis.addons.fairness.post_export_metrics import fairness_indicators\n",
    "from tensorflow_model_analysis.addons.fairness.view import widget_view\n",
    "import sys\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,  CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from tensorflow.python.keras import models\n",
    "# from transformers import BertTokenizer, glue_convert_examples_to_features\n",
    "# from transformers import TFBertForSequenceClassification\n",
    "# import tfrecorder #https://github.com/google/tensorflow-recorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "sys.path.insert(0, os.path.abspath('../')) # needed to import src\n",
    "\n",
    "is_local = True #@param {type:\"boolean\"}\n",
    "local_data_path = '../data/twitter_datasets/combined_harassment/'\n",
    "gdrive_data_path = 'drive/My\\ Drive/Hate\\ Speech\\ Research/contextualizing-hate-speech-models-with-explanations-master/data/twitter/combined_harassment/'\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "data_path = local_data_path if is_local else gdrive_data_path\n",
    "\n",
    "dev_pd = pd.read_csv(f'{data_path}dev.csv', index_col=None).dropna()\n",
    "train_pd = pd.read_csv(f'{data_path}train.csv', index_col=None).dropna()\n",
    "test_pd = pd.read_csv(f'{data_path}test.csv', index_col=None).dropna()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "LABEL = 'is_harassment'\n",
    "TEXT_FEATURE = 'cleaned_tweet'\n",
    "BATCH_SIZE = 512"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Convert the pandas DataFrames to Datasets.\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices(\n",
    "    (train_pd[TEXT_FEATURE].values,\n",
    "     train_pd.pop(LABEL).values.reshape(-1,1) * 1.0)).batch(BATCH_SIZE)\n",
    "dataset_dev = tf.data.Dataset.from_tensor_slices(\n",
    "    (dev_pd[TEXT_FEATURE].values,\n",
    "     dev_pd.pop(LABEL).values.reshape(-1,1) * 1.0)).batch(BATCH_SIZE)\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices(\n",
    "    (test_pd[TEXT_FEATURE].values,\n",
    "     test_pd.pop(LABEL).values.reshape(-1,1) * 1.0)).batch(BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TFBertForSequenceClassification' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-5-6f3250afdea0>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m#todo\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mmodel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mTFBertForSequenceClassification\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'bert-base-uncased'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[0mtokenizer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mBertTokenizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'bert-base-uncased'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'TFBertForSequenceClassification' is not defined"
     ]
    }
   ],
   "source": [
    "#todo\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [
    "# todo put in util\n",
    "## tokenize to unigram + bigram\n",
    "\n",
    "def strip_punc_hp(s):\n",
    "    return str(s).translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def remove_punctuation_tweet(text_array):\n",
    "    # get rid of punctuation (except periods!)\n",
    "    punctuation_no_period = \"[\" + re.sub(\"\\.\",\"\",string.punctuation) + \"]\"\n",
    "    return np.array([re.sub(punctuation_no_period, \"\", text) for text in text_array])\n",
    "\n",
    "def tfidf_vectorize(train_texts: np.ndarray,\n",
    "                    train_labels: np.ndarray,\n",
    "                    val_texts: np.ndarray,\n",
    "                    test_texts: np.ndarray,\n",
    "                    ngram_range: tuple = (1,2),\n",
    "                    top_k: int = 20000,\n",
    "                    token_mode: str = 'word',\n",
    "                    min_document_frequency: int = 2,\n",
    "                    tf_idf: bool = True) -> tuple:\n",
    "    \"\"\"\n",
    "    Vectorizes texts as n-gram vectors.\n",
    "\n",
    "    1 text = 1 tf-idf vector the length of vocabulary of unigrams + bigrams.\n",
    "\n",
    "    # Arguments\n",
    "        @:param train_texts: list, training text strings.\n",
    "        @:param train_labels: np.ndarray, training labels.\n",
    "        @:param val_texts: list, validation text strings.\n",
    "        @:param ngram_range Range: (inclusive) of n-gram sizes for tokenizing text.\n",
    "        @:param top_k: Limit on the number of features. We use the top 20K features.\n",
    "        @:param token_mode:  Whether text should be split into word or character n-grams. One of 'word', 'char'.\n",
    "        @:param min_document_frequency: Minimum document/corpus frequency below which a token will be discarded.\n",
    "\n",
    "    # Returns\n",
    "        x_train, x_val: vectorized training and validation texts\n",
    "\n",
    "    # adapted from: https://developers.google.com/machine-learning/guides/text-classification/step-3\n",
    "    \"\"\"\n",
    "    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
    "    kwargs = {\n",
    "            'ngram_range': ngram_range,\n",
    "            'dtype': 'int32',\n",
    "            'strip_accents': 'unicode',\n",
    "            'decode_error': 'replace',\n",
    "            'analyzer': token_mode,\n",
    "            'min_df': min_document_frequency,\n",
    "    }\n",
    "\n",
    "    vectorizer = TfidfVectorizer(**kwargs) if tf_idf else CountVectorizer(**kwargs)\n",
    "    train_texts = remove_punctuation_tweet(train_texts)\n",
    "    val_texts = remove_punctuation_tweet(val_texts)\n",
    "    test_texts = remove_punctuation_tweet(test_texts)\n",
    "    # Learn vocabulary from training texts and vectorize training texts.\n",
    "    x_train = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "    # Vectorize validation and test texts.\n",
    "    x_val = vectorizer.transform(val_texts)\n",
    "    x_test = vectorizer.transform(test_texts)\n",
    "\n",
    "\n",
    "    # Select top 'k' of the vectorized features.\n",
    "    selector = SelectKBest(f_classif, k=min(top_k, x_train.shape[1]))\n",
    "    selector.fit(x_train, train_labels)\n",
    "    x_train = selector.transform(x_train).astype('float32')\n",
    "    x_val = selector.transform(x_val).astype('float32')\n",
    "    x_test = selector.transform(x_test).astype('float32')\n",
    "\n",
    "    return x_train, x_val, x_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [],
   "source": [
    "def logistic_regression_model():\n",
    "    return models.Sequential([\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "def train_ngram_logreg(train_data,\n",
    "                       validation_data,\n",
    "                       test_data,\n",
    "                       learning_rate=1e-3,\n",
    "                       epochs=1000,\n",
    "                       batch_size=512,\n",
    "                       tf_idf=True,\n",
    "                       ngram_range=(1,2)):\n",
    "    \"\"\"\n",
    "\n",
    "    :param train_data: pandas dataframe of the training data\n",
    "    :param validation_data: pandas dataframe of the validation data\n",
    "    :param test_data: pandas dataframe of the test data\n",
    "    :param learning_rate: float, learning rate for training model.\n",
    "    :param epochs: int, number of epochs.\n",
    "    :param batch_size: int, number of samples per batch.\n",
    "    :param tf_idf: bool, whether to encode tf-idf or n-gram\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    train_texts, y_train = train_data[TEXT_FEATURE].values, train_data[LABEL].values\n",
    "    dev_texts, y_dev = validation_data[TEXT_FEATURE].values, validation_data[LABEL].values\n",
    "    test_texts, y_test = test_data[TEXT_FEATURE].values, test_data[LABEL].values\n",
    "\n",
    "    x_train, x_dev, x_test = tfidf_vectorize(train_texts=train_texts,\n",
    "                                             train_labels=y_train,\n",
    "                                             val_texts=dev_texts,\n",
    "                                             test_texts=test_texts,\n",
    "                                             tf_idf=tf_idf,\n",
    "                                             ngram_range=ngram_range)\n",
    "    model = logistic_regression_model()\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(loss='bce', optimizer=optimizer,metrics=['acc'])\n",
    "    # early stopping if validation loss does not decrease in 2 consecutive tries.\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=2)]\n",
    "    history = model.fit(x_train,\n",
    "              y_train,\n",
    "              epochs=epochs,\n",
    "              callbacks=callbacks,\n",
    "              validation_data=(x_dev, y_dev),\n",
    "              verbose=2, #once per epoch\n",
    "              batch_size=batch_size,\n",
    "              )\n",
    "\n",
    "    # Print results.\n",
    "    history = history.history\n",
    "    val_acc = history[\"val_acc\"][-1]\n",
    "    print(f'Validation accuracy: {val_acc}, loss: {history[\"val_loss\"][-1]}')\n",
    "\n",
    "    y_hat_test = model.predict(x_test)\n",
    "    test_acc = np.mean(y_hat_test == y_test)\n",
    "    print(f'Test accuracy: {test_acc}')\n",
    "\n",
    "    # Save model.\n",
    "    model.save(f'../models/mindiff/n_gram_logreg_lr_{strip_punc_hp(learning_rate)}_batch_{str(batch_size)}_valacc_{str(strip_punc_hp(val_acc))}_testacc_{str(strip_punc_hp(test_acc))}.h5')\n",
    "    return val_acc, history['val_loss'][-1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "211/211 - 1s - loss: 0.5917 - acc: 0.7499 - val_loss: 0.5303 - val_acc: 0.8054\n",
      "Epoch 2/1000\n",
      "211/211 - 1s - loss: 0.4853 - acc: 0.8300 - val_loss: 0.4668 - val_acc: 0.8372\n",
      "Epoch 3/1000\n",
      "211/211 - 1s - loss: 0.4330 - acc: 0.8503 - val_loss: 0.4305 - val_acc: 0.8489\n",
      "Epoch 4/1000\n",
      "211/211 - 1s - loss: 0.3998 - acc: 0.8615 - val_loss: 0.4064 - val_acc: 0.8562\n",
      "Epoch 5/1000\n",
      "211/211 - 1s - loss: 0.3760 - acc: 0.8690 - val_loss: 0.3888 - val_acc: 0.8621\n",
      "Epoch 6/1000\n",
      "211/211 - 1s - loss: 0.3580 - acc: 0.8755 - val_loss: 0.3754 - val_acc: 0.8655\n",
      "Epoch 7/1000\n",
      "211/211 - 1s - loss: 0.3436 - acc: 0.8804 - val_loss: 0.3646 - val_acc: 0.8680\n",
      "Epoch 8/1000\n",
      "211/211 - 1s - loss: 0.3317 - acc: 0.8843 - val_loss: 0.3560 - val_acc: 0.8707\n",
      "Epoch 9/1000\n",
      "211/211 - 1s - loss: 0.3217 - acc: 0.8877 - val_loss: 0.3485 - val_acc: 0.8743\n",
      "Epoch 10/1000\n",
      "211/211 - 1s - loss: 0.3130 - acc: 0.8904 - val_loss: 0.3424 - val_acc: 0.8767\n",
      "Epoch 11/1000\n",
      "211/211 - 2s - loss: 0.3055 - acc: 0.8931 - val_loss: 0.3371 - val_acc: 0.8788\n",
      "Epoch 12/1000\n",
      "211/211 - 1s - loss: 0.2989 - acc: 0.8956 - val_loss: 0.3325 - val_acc: 0.8802\n",
      "Epoch 13/1000\n",
      "211/211 - 1s - loss: 0.2930 - acc: 0.8974 - val_loss: 0.3285 - val_acc: 0.8820\n",
      "Epoch 14/1000\n",
      "211/211 - 1s - loss: 0.2877 - acc: 0.8991 - val_loss: 0.3250 - val_acc: 0.8842\n",
      "Epoch 15/1000\n",
      "211/211 - 2s - loss: 0.2829 - acc: 0.9002 - val_loss: 0.3219 - val_acc: 0.8854\n",
      "Epoch 16/1000\n",
      "211/211 - 1s - loss: 0.2786 - acc: 0.9017 - val_loss: 0.3193 - val_acc: 0.8867\n",
      "Epoch 17/1000\n",
      "211/211 - 2s - loss: 0.2747 - acc: 0.9027 - val_loss: 0.3170 - val_acc: 0.8874\n",
      "Epoch 18/1000\n",
      "211/211 - 1s - loss: 0.2711 - acc: 0.9038 - val_loss: 0.3149 - val_acc: 0.8877\n",
      "Epoch 19/1000\n",
      "211/211 - 2s - loss: 0.2678 - acc: 0.9045 - val_loss: 0.3131 - val_acc: 0.8886\n",
      "Epoch 20/1000\n",
      "211/211 - 1s - loss: 0.2648 - acc: 0.9052 - val_loss: 0.3116 - val_acc: 0.8892\n",
      "Epoch 21/1000\n",
      "211/211 - 1s - loss: 0.2621 - acc: 0.9060 - val_loss: 0.3102 - val_acc: 0.8906\n",
      "Epoch 22/1000\n",
      "211/211 - 1s - loss: 0.2595 - acc: 0.9067 - val_loss: 0.3091 - val_acc: 0.8906\n",
      "Epoch 23/1000\n",
      "211/211 - 1s - loss: 0.2571 - acc: 0.9074 - val_loss: 0.3082 - val_acc: 0.8904\n",
      "Epoch 24/1000\n",
      "211/211 - 1s - loss: 0.2549 - acc: 0.9080 - val_loss: 0.3072 - val_acc: 0.8915\n",
      "Epoch 25/1000\n",
      "211/211 - 1s - loss: 0.2529 - acc: 0.9085 - val_loss: 0.3064 - val_acc: 0.8917\n",
      "Epoch 26/1000\n",
      "211/211 - 1s - loss: 0.2509 - acc: 0.9092 - val_loss: 0.3059 - val_acc: 0.8922\n",
      "Epoch 27/1000\n",
      "211/211 - 1s - loss: 0.2492 - acc: 0.9096 - val_loss: 0.3054 - val_acc: 0.8923\n",
      "Epoch 28/1000\n",
      "211/211 - 1s - loss: 0.2475 - acc: 0.9099 - val_loss: 0.3050 - val_acc: 0.8924\n",
      "Epoch 29/1000\n",
      "211/211 - 1s - loss: 0.2459 - acc: 0.9104 - val_loss: 0.3046 - val_acc: 0.8928\n",
      "Epoch 30/1000\n",
      "211/211 - 1s - loss: 0.2444 - acc: 0.9108 - val_loss: 0.3044 - val_acc: 0.8926\n",
      "Epoch 31/1000\n",
      "211/211 - 1s - loss: 0.2430 - acc: 0.9111 - val_loss: 0.3043 - val_acc: 0.8928\n",
      "Epoch 32/1000\n",
      "211/211 - 1s - loss: 0.2417 - acc: 0.9115 - val_loss: 0.3043 - val_acc: 0.8926\n",
      "Epoch 33/1000\n",
      "211/211 - 1s - loss: 0.2404 - acc: 0.9118 - val_loss: 0.3041 - val_acc: 0.8929\n",
      "Epoch 34/1000\n",
      "211/211 - 1s - loss: 0.2392 - acc: 0.9122 - val_loss: 0.3042 - val_acc: 0.8927\n",
      "Epoch 35/1000\n",
      "211/211 - 1s - loss: 0.2381 - acc: 0.9123 - val_loss: 0.3043 - val_acc: 0.8923\n",
      "Validation accuracy: 0.8923420310020447, loss: 0.3042904734611511\n",
      "Test accuracy: 0.000437065290925986\n"
     ]
    },
    {
     "data": {
      "text/plain": "(0.8923420310020447, 0.3042904734611511)"
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ngram_logreg(train_data=train_pd, validation_data=dev_pd, test_data=test_pd, tf_idf=False, ngram_range=(1,2))\n",
    "train_ngram_logreg(train_data=train_pd, validation_data=dev_pd, test_data=test_pd, tf_idf=True, ngram_range=(1,2))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "def n_gram_vectorize(train_texts: np.ndarray,\n",
    "                     train_labels: np.ndarray,\n",
    "                     val_texts: np.ndarray,\n",
    "                     ngrams: tuple =(1,1),\n",
    "                     top_k: int=20000) -> tuple:\n",
    "\n",
    "    # Instantiate TextVectorization with \"binary\" output_mode (multi-hot)\n",
    "    # todo experiment with bigram/unigram\n",
    "    vectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(ngrams=ngrams, output_mode='binary', max_tokens=top_k)\n",
    "    vectorize_layer.adapt(train_texts)\n",
    "\n",
    "    x_train = vectorize_layer(train_texts).numpy()\n",
    "    x_dev = vectorize_layer(val_texts).numpy()\n",
    "\n",
    "    # Select top 'k' of the vectorized features.\n",
    "    selector = SelectKBest(f_classif, k=min(top_k, x_train.shape[1]))\n",
    "    selector.fit(x_train, train_labels)\n",
    "    x_train = selector.transform(x_train).astype('float32')\n",
    "    x_val = selector.transform(x_dev).astype('float32')\n",
    "    x_train, x_val\n",
    "\n",
    "n_gram_vectorize(train_texts=train_pd[TEXT_FEATURE].values,\n",
    "                 train_labels=train_pd[LABEL].values,\n",
    "                 val_texts=dev_pd[TEXT_FEATURE].values,)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}